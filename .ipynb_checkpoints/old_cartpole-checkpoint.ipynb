{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# observation = tflearn.input_data(shape=[None, 4])\n",
    "# net = tflearn.fully_connected(observation, 256, activation=\"relu\")\n",
    "# net = tflearn.fully_connected(net, 256, activation=\"relu\")\n",
    "# net = tflearn.fully_connected(net, 256, activation=\"relu\")\n",
    "# out = tflearn.fully_connected(net, 2, activation=\"softmax\")\n",
    "\n",
    "observation = tf.placeholder(tf.float64, shape=[None, 4])\n",
    "dense1 = dense(observation, 128, activation=tf.tanh)\n",
    "dense2 = dense(dense1, 128, activation=tf.tanh)\n",
    "act_probs = dense(dense2, 2)\n",
    "softmax_probs = tf.nn.softmax(act_probs)\n",
    "chosen_act = tf.squeeze(tf.multinomial(logits=softmax_probs,num_samples=1), axis=1)\n",
    "\n",
    "reward_holder = tf.placeholder(tf.float64, [None])\n",
    "action_holder = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "responsible_outputs = tf.gather(tf.reshape(softmax_probs, [-1]), tf.range(0, tf.shape(softmax_probs)[0] * tf.shape(softmax_probs)[1], 2) + action_holder)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs) * reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, gamma=0.99):\n",
    "    new_rewards = [rewards[-1]]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(rewards[i] + gamma * new_rewards[-1])\n",
    "    return new_rewards[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1\n",
    "    \n",
    "    def start_rollout(self):\n",
    "        self.rollout_idx = (self.rollout_idx + 1) % self.max_size\n",
    "        if self.rollout_idx >= len(self.rollouts):\n",
    "            self.rollouts.append([])\n",
    "        else:\n",
    "            self.rollouts[self.rollout_idx] = []\n",
    "            \n",
    "    def end_rollout(self):\n",
    "        self.start_rollout()\n",
    "    \n",
    "    def record(self, obs, act, rew):\n",
    "        self.rollouts[self.rollout_idx].append([obs, act, rew])\n",
    "        \n",
    "    def to_data(self, reset=True):\n",
    "        all_data = []\n",
    "        \n",
    "        try:\n",
    "            for rollout in self.rollouts:\n",
    "                rollout = np.array(rollout)\n",
    "                # Discount the rewards for every rollout\n",
    "                rollout[:,2] = discount_reward(rollout[:,2])\n",
    "                all_data.extend(list(rollout))\n",
    "\n",
    "            if reset:\n",
    "                self.reset()\n",
    "        except IndexError:\n",
    "            return np.array([])\n",
    "            \n",
    "        return np.array(all_data)\n",
    "                \n",
    "    def reset(self):\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.88\n",
      "103.47\n",
      "126.39\n",
      "142.23\n",
      "152.15\n",
      "159.01\n",
      "162.67\n",
      "163.7\n",
      "163.18\n",
      "165.66\n",
      "164.09\n",
      "165.49\n",
      "166.75\n",
      "170.72\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1500\n",
    "max_time = 200\n",
    "all_rewards = []\n",
    "saver = tf.train.Saver()\n",
    "train_data = []\n",
    "\n",
    "mb = MemoryBuffer(100) ##############\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        ep_history = []\n",
    "        mb.start_rollout() ###########\n",
    "        for j in range(max_time):\n",
    "            #Choose an action\n",
    "#             a_one_hot = sess.run(softmax_probs, feed_dict={observation: [obs]}).reshape(2)\n",
    "#             action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "#             action = np.argmax(a_one_hot == action)\n",
    "            action = sess.run(softmax_probs, feed_dict={observation: [obs]}).reshape(2)\n",
    "            obs1, r, d, _ = env.step(action)\n",
    "            ep_history.append([obs, r, action])\n",
    "            mb.record(obs, action, r) #################\n",
    "            obs = obs1\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                all_rewards.append(episode_reward)\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 1] = discount_reward(ep_history[:, 1])\n",
    "                train_data.extend(ep_history)\n",
    "                \n",
    "                if i % 10 == 0 and i != 0:\n",
    "                    rollouts = mb.to_data() #####################\n",
    "                    train_data = np.array(train_data)\n",
    "                    sess.run(update, feed_dict={observation: np.vstack(rollouts[:, 0]), #############\n",
    "                                                    reward_holder: rollouts[:, 2],      #############\n",
    "                                                    action_holder: rollouts[:, 1]})     #############\n",
    "                    train_data = []\n",
    "                break\n",
    "                \n",
    "        if i % 100 == 0 and i != 0:\n",
    "            print(np.mean(all_rewards[-100:]))\n",
    "            if np.mean(all_rewards[-100:]) == 200:\n",
    "                break\n",
    "            \n",
    "    saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
