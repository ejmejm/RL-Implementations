{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cart_pole():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    new_rewards = [rewards[-1]]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(rewards[i] + gamma * new_rewards[-1])\n",
    "    return new_rewards[::-1]\n",
    "\n",
    "class MemoryBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1\n",
    "    \n",
    "    def start_rollout(self):\n",
    "        self.rollout_idx = (self.rollout_idx + 1) % self.max_size\n",
    "        if self.rollout_idx >= len(self.rollouts):\n",
    "            self.rollouts.append([])\n",
    "        else:\n",
    "            self.rollouts[self.rollout_idx] = []\n",
    "            \n",
    "    def end_rollout(self):\n",
    "        self.start_rollout()\n",
    "    \n",
    "    def record(self, obs, act, rew):\n",
    "        self.rollouts[self.rollout_idx].append([obs, act, rew])\n",
    "        \n",
    "    def to_data(self, reset=True):\n",
    "        all_data = []\n",
    "        \n",
    "        try:\n",
    "            for rollout in self.rollouts:\n",
    "                rollout = np.array(rollout)\n",
    "                # Discount the rewards for every rollout\n",
    "                rollout[:,2] = discount_rewards(rollout[:,2])\n",
    "                all_data.extend(list(rollout))\n",
    "\n",
    "            if reset:\n",
    "                self.reset()\n",
    "        except IndexError:\n",
    "            return np.array([])\n",
    "            \n",
    "        return np.array(all_data)\n",
    "                \n",
    "    def reset(self):\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, obs_shape, act_space, sess=None):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.act_space = act_space\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.build_network()\n",
    "        \n",
    "    def build_network(self):\n",
    "        self.obs = tf.placeholder(tf.float64, shape=[None, self.obs_shape])\n",
    "        self.dense1 = dense(self.obs, 128, activation=tf.tanh)\n",
    "        self.dense2 = dense(self.dense1, 128, activation=tf.tanh)\n",
    "        self.act_probs = dense(self.dense2, self.act_space)\n",
    "        self.softmax_probs = tf.nn.softmax(self.act_probs)\n",
    "        self.chosen_act = tf.squeeze(tf.multinomial(logits=self.softmax_probs,num_samples=1), axis=1)\n",
    "        self.actions = tf.placeholder(tf.int64, shape=[None])\n",
    "        self.rewards = tf.placeholder(tf.float64, shape=[None])\n",
    "        \n",
    "        self.action_masks = tf.one_hot(self.actions, self.act_space, dtype=tf.float64)\n",
    "        self.log_probs = tf.log(self.softmax_probs)\n",
    "        \n",
    "        self.resp_actions = tf.reduce_sum(self.action_masks *  self.log_probs, axis=1)\n",
    "        self.loss = -tf.reduce_mean(self.resp_actions * self.rewards)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "#         self.action_masks = tf.one_hot(self.actions, self.act_space, dtype=tf.float64)\n",
    "#         self.resp_actions = tf.reduce_sum(self.action_masks * tf.log(self.out), axis=1)\n",
    "        \n",
    "#         self.loss = -tf.reduce_mean(self.resp_actions * self.rewards)\n",
    "        \n",
    "# #         self.loss = -tf.reduce_mean(tf.transpose(tf.log(self.resp_actions)) * self.rewards)\n",
    "#         self.optimizer = tf.train.AdamOptimizer()\n",
    "#         self.optimize = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.__enter__()\n",
    "        return self.sess\n",
    "    \n",
    "    def end_session(self):\n",
    "        if self.sess is not None:\n",
    "            self.sess.close()\n",
    "        \n",
    "#     def choose_action(self, obs, sess=None):\n",
    "#         if not sess:\n",
    "#             if self.sess is not None:\n",
    "#                 sess = self.sess\n",
    "#             else:\n",
    "#                 sess = tf.get_default_session()\n",
    "#                 assert sess is not None\n",
    "        \n",
    "#         act = sess.run(self.chosen_act, feed_dict={self.obs: obs})\n",
    "#         return act\n",
    "    \n",
    "    def choose_action(self, obs, sess=None):\n",
    "        if not sess:\n",
    "            if self.sess is not None:\n",
    "                sess = self.sess\n",
    "            else:\n",
    "                sess = tf.get_default_session()\n",
    "                assert sess is not None\n",
    "        \n",
    "        probs = sess.run(self.softmax_probs, feed_dict={self.obs: obs})\n",
    "        act = np.random.choice(list(range(len(probs)+1)), p=probs[0])\n",
    "        \n",
    "        return act\n",
    "    \n",
    "    def train(self, train_data, sess=None):\n",
    "        \"\"\"train_data: in the two-dimensional numpy array with the format, [obs, act, rew]\"\"\"\n",
    "        if not sess:\n",
    "            if self.sess is not None:\n",
    "                sess = self.sess\n",
    "            else:\n",
    "                sess = tf.get_default_session()\n",
    "                assert sess is not None\n",
    "        \n",
    "        a, b, _ = sess.run([self.softmax_probs, tf.multinomial(logits=self.softmax_probs,num_samples=1), self.update], feed_dict={self.obs: np.vstack(train_data[:,0]),\n",
    "                                            self.actions: train_data[:,1],\n",
    "                                            self.rewards: train_data[:,2]})\n",
    "#         print(a[:50])\n",
    "#         print(b[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "max_steps = 200\n",
    "update_freq = 200 # In episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MemoryBuffer(update_freq)\n",
    "env = make_cart_pole()\n",
    "network = Network(env.observation_space.shape[0], env.action_space.n)\n",
    "network.init_session();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Reward: 22.5\n",
      "Total Episodes: 100\n",
      "Total Steps: 2250\n",
      "\n",
      "-----------\n",
      "Recent Reward: 30.51\n",
      "Total Episodes: 200\n",
      "Total Steps: 5301\n",
      "\n",
      "-----------\n",
      "Recent Reward: 29.68\n",
      "Total Episodes: 300\n",
      "Total Steps: 8269\n",
      "\n",
      "-----------\n",
      "Recent Reward: 41.41\n",
      "Total Episodes: 400\n",
      "Total Steps: 12410\n",
      "\n",
      "-----------\n",
      "Recent Reward: 42.8\n",
      "Total Episodes: 500\n",
      "Total Steps: 16690\n",
      "\n",
      "-----------\n",
      "Recent Reward: 47.44\n",
      "Total Episodes: 600\n",
      "Total Steps: 21434\n",
      "\n",
      "-----------\n",
      "Recent Reward: 54.51\n",
      "Total Episodes: 700\n",
      "Total Steps: 26885\n",
      "\n",
      "-----------\n",
      "Recent Reward: 59.65\n",
      "Total Episodes: 800\n",
      "Total Steps: 32850\n",
      "\n",
      "-----------\n",
      "Recent Reward: 66.6\n",
      "Total Episodes: 900\n",
      "Total Steps: 39510\n",
      "\n",
      "-----------\n",
      "Recent Reward: 72.69\n",
      "Total Episodes: 1000\n",
      "Total Steps: 46779\n",
      "\n",
      "-----------\n",
      "Recent Reward: 76.72\n",
      "Total Episodes: 1100\n",
      "Total Steps: 54451\n",
      "\n",
      "-----------\n",
      "Recent Reward: 82.33\n",
      "Total Episodes: 1200\n",
      "Total Steps: 62684\n",
      "\n",
      "-----------\n",
      "Recent Reward: 95.47\n",
      "Total Episodes: 1300\n",
      "Total Steps: 72231\n",
      "\n",
      "-----------\n",
      "Recent Reward: 112.11\n",
      "Total Episodes: 1400\n",
      "Total Steps: 83442\n",
      "\n",
      "-----------\n",
      "Recent Reward: 135.48\n",
      "Total Episodes: 1500\n",
      "Total Steps: 96990\n",
      "\n",
      "-----------\n",
      "Recent Reward: 148.4\n",
      "Total Episodes: 1600\n",
      "Total Steps: 111830\n",
      "\n",
      "-----------\n",
      "Recent Reward: 148.39\n",
      "Total Episodes: 1700\n",
      "Total Steps: 126669\n",
      "\n",
      "-----------\n",
      "Recent Reward: 160.6\n",
      "Total Episodes: 1800\n",
      "Total Steps: 142729\n",
      "\n",
      "-----------\n",
      "Recent Reward: 172.34\n",
      "Total Episodes: 1900\n",
      "Total Steps: 159963\n",
      "\n",
      "-----------\n",
      "Recent Reward: 172.36\n",
      "Total Episodes: 2000\n",
      "Total Steps: 177199\n",
      "\n",
      "-----------\n",
      "Recent Reward: 177.89\n",
      "Total Episodes: 2100\n",
      "Total Steps: 194988\n",
      "\n",
      "-----------\n",
      "Recent Reward: 186.97\n",
      "Total Episodes: 2200\n",
      "Total Steps: 213685\n",
      "\n",
      "-----------\n",
      "Recent Reward: 187.28\n",
      "Total Episodes: 2300\n",
      "Total Steps: 232413\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.82\n",
      "Total Episodes: 2400\n",
      "Total Steps: 251795\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.34\n",
      "Total Episodes: 2500\n",
      "Total Steps: 271329\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.2\n",
      "Total Episodes: 2600\n",
      "Total Steps: 290649\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.43\n",
      "Total Episodes: 2700\n",
      "Total Steps: 310092\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.08\n",
      "Total Episodes: 2800\n",
      "Total Steps: 329600\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.41\n",
      "Total Episodes: 2900\n",
      "Total Steps: 348941\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.46\n",
      "Total Episodes: 3000\n",
      "Total Steps: 368487\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.5\n",
      "Total Episodes: 3100\n",
      "Total Steps: 387937\n",
      "\n",
      "-----------\n",
      "Recent Reward: 192.33\n",
      "Total Episodes: 3200\n",
      "Total Steps: 407170\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.54\n",
      "Total Episodes: 3300\n",
      "Total Steps: 426524\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.98\n",
      "Total Episodes: 3400\n",
      "Total Steps: 446122\n",
      "\n",
      "-----------\n",
      "Recent Reward: 196.58\n",
      "Total Episodes: 3500\n",
      "Total Steps: 465780\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.53\n",
      "Total Episodes: 3600\n",
      "Total Steps: 485333\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.36\n",
      "Total Episodes: 3700\n",
      "Total Steps: 504869\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.72\n",
      "Total Episodes: 3800\n",
      "Total Steps: 524241\n",
      "\n",
      "-----------\n",
      "Recent Reward: 193.37\n",
      "Total Episodes: 3900\n",
      "Total Steps: 543578\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.96\n",
      "Total Episodes: 4000\n",
      "Total Steps: 563074\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.22\n",
      "Total Episodes: 4100\n",
      "Total Steps: 582496\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.32\n",
      "Total Episodes: 4200\n",
      "Total Steps: 601928\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.44\n",
      "Total Episodes: 4300\n",
      "Total Steps: 621472\n",
      "\n",
      "-----------\n",
      "Recent Reward: 196.08\n",
      "Total Episodes: 4400\n",
      "Total Steps: 641080\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.0\n",
      "Total Episodes: 4500\n",
      "Total Steps: 660780\n",
      "\n",
      "-----------\n",
      "Recent Reward: 194.86\n",
      "Total Episodes: 4600\n",
      "Total Steps: 680266\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.91\n",
      "Total Episodes: 4700\n",
      "Total Steps: 700157\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.84\n",
      "Total Episodes: 4800\n",
      "Total Steps: 720041\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.81\n",
      "Total Episodes: 4900\n",
      "Total Steps: 740022\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.88\n",
      "Total Episodes: 5000\n",
      "Total Steps: 760010\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.71\n",
      "Total Episodes: 5100\n",
      "Total Steps: 779881\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.46\n",
      "Total Episodes: 5200\n",
      "Total Steps: 799827\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.72\n",
      "Total Episodes: 5300\n",
      "Total Steps: 819799\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.85\n",
      "Total Episodes: 5400\n",
      "Total Steps: 839784\n",
      "\n",
      "-----------\n",
      "Recent Reward: 200.0\n",
      "Total Episodes: 5500\n",
      "Total Steps: 859784\n",
      "\n",
      "-----------\n",
      "Recent Reward: 200.0\n",
      "Total Episodes: 5600\n",
      "Total Steps: 879784\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.69\n",
      "Total Episodes: 5700\n",
      "Total Steps: 899753\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.16\n",
      "Total Episodes: 5800\n",
      "Total Steps: 919669\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.87\n",
      "Total Episodes: 5900\n",
      "Total Steps: 939556\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.4\n",
      "Total Episodes: 6000\n",
      "Total Steps: 959496\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.78\n",
      "Total Episodes: 6100\n",
      "Total Steps: 979474\n",
      "\n",
      "-----------\n",
      "Recent Reward: 200.0\n",
      "Total Episodes: 6200\n",
      "Total Steps: 999474\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.54\n",
      "Total Episodes: 6300\n",
      "Total Steps: 1019328\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.01\n",
      "Total Episodes: 6400\n",
      "Total Steps: 1039229\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.47\n",
      "Total Episodes: 6500\n",
      "Total Steps: 1059176\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.29\n",
      "Total Episodes: 6600\n",
      "Total Steps: 1079105\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.66\n",
      "Total Episodes: 6700\n",
      "Total Steps: 1099071\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.47\n",
      "Total Episodes: 6800\n",
      "Total Steps: 1119018\n",
      "\n",
      "-----------\n",
      "Recent Reward: 200.0\n",
      "Total Episodes: 6900\n",
      "Total Steps: 1139018\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.69\n",
      "Total Episodes: 7000\n",
      "Total Steps: 1158887\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.98\n",
      "Total Episodes: 7100\n",
      "Total Steps: 1178885\n",
      "\n",
      "-----------\n",
      "Recent Reward: 200.0\n",
      "Total Episodes: 7200\n",
      "Total Steps: 1198885\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.72\n",
      "Total Episodes: 7300\n",
      "Total Steps: 1218857\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.38\n",
      "Total Episodes: 7400\n",
      "Total Steps: 1238795\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.69\n",
      "Total Episodes: 7500\n",
      "Total Steps: 1258764\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.26\n",
      "Total Episodes: 7600\n",
      "Total Steps: 1278690\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.93\n",
      "Total Episodes: 7700\n",
      "Total Steps: 1298683\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.72\n",
      "Total Episodes: 7800\n",
      "Total Steps: 1318655\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.69\n",
      "Total Episodes: 7900\n",
      "Total Steps: 1338624\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.93\n",
      "Total Episodes: 8000\n",
      "Total Steps: 1358617\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.36\n",
      "Total Episodes: 8100\n",
      "Total Steps: 1378553\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.23\n",
      "Total Episodes: 8200\n",
      "Total Steps: 1398476\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.99\n",
      "Total Episodes: 8300\n",
      "Total Steps: 1418375\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.79\n",
      "Total Episodes: 8400\n",
      "Total Steps: 1438254\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.18\n",
      "Total Episodes: 8500\n",
      "Total Steps: 1458072\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.33\n",
      "Total Episodes: 8600\n",
      "Total Steps: 1477905\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.05\n",
      "Total Episodes: 8700\n",
      "Total Steps: 1497710\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.41\n",
      "Total Episodes: 8800\n",
      "Total Steps: 1517551\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.78\n",
      "Total Episodes: 8900\n",
      "Total Steps: 1537329\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.08\n",
      "Total Episodes: 9000\n",
      "Total Steps: 1557137\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.46\n",
      "Total Episodes: 9100\n",
      "Total Steps: 1576883\n",
      "\n",
      "-----------\n",
      "Recent Reward: 199.33\n",
      "Total Episodes: 9200\n",
      "Total Steps: 1596816\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.94\n",
      "Total Episodes: 9300\n",
      "Total Steps: 1616610\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.49\n",
      "Total Episodes: 9400\n",
      "Total Steps: 1636459\n",
      "\n",
      "-----------\n",
      "Recent Reward: 198.66\n",
      "Total Episodes: 9500\n",
      "Total Steps: 1656325\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.73\n",
      "Total Episodes: 9600\n",
      "Total Steps: 1676098\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.23\n",
      "Total Episodes: 9700\n",
      "Total Steps: 1695821\n",
      "\n",
      "-----------\n",
      "Recent Reward: 197.03\n",
      "Total Episodes: 9800\n",
      "Total Steps: 1715524\n",
      "\n",
      "-----------\n",
      "Recent Reward: 195.22\n",
      "Total Episodes: 9900\n",
      "Total Steps: 1735046\n",
      "\n",
      "-----------\n",
      "Recent Reward: 191.72\n",
      "Total Episodes: 10000\n",
      "Total Steps: 1754218\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "total_steps = 0\n",
    "total_episodes = 0\n",
    "all_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    mb.start_rollout()\n",
    "    for step in range(max_steps):\n",
    "        act = network.choose_action([obs])\n",
    "\n",
    "        # print(act)\n",
    "        # env.render()\n",
    "        # time.sleep(0.01)\n",
    "        obs_next, rew, d, _ = env.step(act)\n",
    "        episode_reward += rew\n",
    "        \n",
    "        mb.record(obs, act, rew)\n",
    "        obs = obs_next\n",
    "        \n",
    "        total_steps += 1\n",
    "        if d: # or total_steps % update_freq == 0:\n",
    "            #  print(act)\n",
    "            break\n",
    "    # print(episode_reward)\n",
    "    all_rewards.append(episode_reward)\n",
    "    total_episodes += 1\n",
    "            \n",
    "    if total_episodes % 100 == 0:\n",
    "        print('Recent Reward:', np.mean(all_rewards[-100:]))\n",
    "        print('Total Episodes:', total_episodes)\n",
    "        print('Total Steps:', total_steps)\n",
    "        print('\\n-----------')\n",
    "        \n",
    "        train_data = mb.to_data()\n",
    "#         rews = train_data[:,2]\n",
    "#         train_data[:,2] = (rews - np.mean(rews)) / np.std(rews)\n",
    "        network.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.end_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.to_data().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
