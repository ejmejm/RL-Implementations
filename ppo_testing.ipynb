{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import MemoryBuffer\n",
    "from policies import PPOTrainer\n",
    "from utils import gaussian_likelihood, reshape_train_var\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense, conv2d, max_pooling2d, flatten\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from env import CartPoleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self, in_op, out_op, value_out_op, act_type='discrete', sess=None, clip_val=0.2, ppo_iters=80,\n",
    "                 target_kl=0.01, v_coef=1., entropy_coef=0.01):\n",
    "        \"\"\"\n",
    "        Create a wrapper for RL networks for easy training.\n",
    "        Args:\n",
    "            in_op (tf.Placeholder): Observation input to architecture\n",
    "            out_op (tf.Variable): Action output of architecture\n",
    "            act_type (string): 'discrete' for a discrete actions space or 'continuous'\n",
    "                               for a continuous actions space\n",
    "            sess (tf.Session): A session if you would like to use a custom session,\n",
    "                               if left none it will be automatically created\n",
    "        \"\"\"\n",
    "\n",
    "        if not sess:\n",
    "            self.renew_sess()\n",
    "        \n",
    "        self.in_op = in_op\n",
    "        self.out_op = out_op\n",
    "        self.value_out_op = value_out_op\n",
    "        self._prev_weights = None\n",
    "        self.clip_val = clip_val\n",
    "        self.ppo_iters = ppo_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.v_coef = v_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        if act_type in ('discrete', 'd'):\n",
    "            self.train = self._create_discrete_trainer()\n",
    "            self.act_type = 'discrete'\n",
    "        elif act_type in ('continuous', 'c'):\n",
    "            self.train = self._create_continuous_trainer()\n",
    "            self.act_type = 'continuous'\n",
    "        else:\n",
    "            raise TypeError('act_type must be \\'discrete\\' or \\'continuous\\'')\n",
    "        \n",
    "    def renew_sess(self):\n",
    "        \"\"\"\n",
    "        Starts a new internal Tensorflow session\n",
    "        \"\"\"\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def end_sess(self):\n",
    "        \"\"\"\n",
    "        Ends the internal Tensorflow session if it exists\n",
    "        \"\"\"\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        \n",
    "    def _create_discrete_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a discrete action space\n",
    "        \"\"\"\n",
    "        # First passthrough\n",
    "        \n",
    "        self.act_holders = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.act_masks = tf.one_hot(self.act_holders, self.out_op.shape[1].value, dtype=tf.float32)\n",
    "        self.resp_acts = tf.reduce_sum(self.act_masks *  self.out_op, axis=1)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.value_out_op)\n",
    "        \n",
    "        # Second passthrough\n",
    "        \n",
    "        self.advatange_holders = tf.placeholder(dtype=tf.float32, shape=self.advantages.shape)\n",
    "        self.old_prob_holders = tf.placeholder(dtype=tf.float32, shape=self.resp_acts.shape)\n",
    " \n",
    "        self.policy_ratio = self.resp_acts / self.old_prob_holders\n",
    "        self.clipped_ratio = tf.clip_by_value(self.policy_ratio, 1 - self.clip_val, 1 + self.clip_val)\n",
    "\n",
    "        self.min_loss = tf.minimum(self.policy_ratio * self.advatange_holders, self.clipped_ratio * self.advatange_holders)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Actor update\n",
    "        \n",
    "        self.kl_divergence = tf.reduce_mean(tf.log(self.old_prob_holders) - tf.log(self.resp_acts))\n",
    "        self.actor_loss = -tf.reduce_mean(self.min_loss)\n",
    "        self.actor_update = self.optimizer.minimize(self.actor_loss)\n",
    "\n",
    "        # Value update\n",
    "        \n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.value_out_op)))\n",
    "        self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        # Combined update\n",
    "        \n",
    "        self.combined_loss = self.actor_loss + 1. * self.value_loss\n",
    "        self.combined_update = self.optimizer.minimize(self.combined_loss)\n",
    "        \n",
    "        def update_func(train_data):\n",
    "            self.old_probs, self.old_advantages = self.sess.run([self.resp_acts, self.advantages], \n",
    "                                    feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                               self.act_holders: train_data[:, 1],\n",
    "                                               self.reward_holders: train_data[:, 2]})\n",
    "        \n",
    "            for i in range(self.ppo_iters):\n",
    "                kl_div, _ = self.sess.run([self.kl_divergence, self.combined_update], \n",
    "                               feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                    self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                    self.reward_holders: train_data[:, 2],\n",
    "                                    self.old_prob_holders: self.old_probs,\n",
    "                                    self.advatange_holders: self.old_advantages})\n",
    "                if kl_div > 1.5 * self.target_kl:\n",
    "                    break\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _create_continuous_trainer(self):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a continuous action space\n",
    "        \"\"\"\n",
    "        # First passthrough\n",
    "        \n",
    "        self.act_holders = tf.placeholder(tf.float32, shape=[None, self.out_op.shape[1].value])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.std = tf.Variable(0.5 * np.ones(shape=self.out_op.shape[1].value), dtype=tf.float32)\n",
    "        self.out_act = self.out_op + tf.random_normal(tf.shape(self.out_op), dtype=tf.float32) * self.std\n",
    "        \n",
    "        self.log_probs = gaussian_likelihood(self.act_holders, self.out_op, self.std)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.value_out_op)\n",
    "        \n",
    "        # Second passthrough\n",
    "        \n",
    "        self.advatange_holders = tf.placeholder(dtype=tf.float32, shape=self.advantages.shape)\n",
    "        self.old_prob_holders = tf.placeholder(dtype=tf.float32, shape=self.log_probs.shape)\n",
    " \n",
    "        self.policy_ratio = tf.exp(self.log_probs - self.old_prob_holders)\n",
    "        self.clipped_ratio = tf.clip_by_value(self.policy_ratio, 1 - self.clip_val, 1 + self.clip_val)\n",
    "\n",
    "        self.min_loss = tf.minimum(self.policy_ratio * self.advatange_holders, self.clipped_ratio * self.advatange_holders)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Actor update\n",
    "        \n",
    "        self.kl_divergence = tf.reduce_mean(self.old_prob_holders - self.log_probs)\n",
    "        self.actor_loss = -tf.reduce_mean(self.min_loss)\n",
    "        self.actor_update = self.optimizer.minimize(self.actor_loss)\n",
    "\n",
    "        # Value update\n",
    "        \n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.value_out_op)))\n",
    "        self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        # Combined update\n",
    "        \n",
    "        self.entropy = -0.5 * tf.reduce_mean(tf.log(2 * np.pi * np.e * self.std))\n",
    "        self.combined_loss = self.actor_loss + self.v_coef * self.value_loss + self.entropy_coef * self.entropy\n",
    "        self.combined_update = self.optimizer.minimize(self.combined_loss)\n",
    "        \n",
    "        def update_func(train_data):\n",
    "            self.old_probs, self.old_advantages = self.sess.run([self.log_probs, self.advantages], \n",
    "                                    feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                               self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                               self.reward_holders: train_data[:, 2]})\n",
    "            \n",
    "            for i in range(self.ppo_iters):\n",
    "                kl_div, _ = self.sess.run([self.kl_divergence, self.combined_update], \n",
    "                               feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                    self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                    self.reward_holders: train_data[:, 2],\n",
    "                                    self.old_prob_holders: self.old_probs,\n",
    "                                    self.advatange_holders: self.old_advantages})\n",
    "                if kl_div > 1.5 * self.target_kl:\n",
    "                    break\n",
    "            \n",
    "            return kl_div, self.sess.run(self.entropy)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _gen_discrete_act(self, obs):\n",
    "        act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
    "        act = np.random.choice(list(range(len(act_probs[0]))), p=act_probs[0])\n",
    "        \n",
    "        return act\n",
    "    \n",
    "    def _gen_continuous_act(self, obs):\n",
    "        act_vect = self.sess.run(self.out_act, feed_dict={self.in_op: [obs]})[0]\n",
    "        \n",
    "        return np.array(act_vect)\n",
    "        \n",
    "    def gen_act(self, obs):\n",
    "        if self.act_type == 'discrete':\n",
    "            return self._gen_discrete_act(obs)\n",
    "        else:\n",
    "            return self._gen_continuous_act(obs)\n",
    "        \n",
    "    def train(self, obs, rewards, acts):\n",
    "        raise RuntimeError('The train method was not properly created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# # env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "# obs = tf.placeholder(tf.float32, shape=[None]+list(env.observation_space.shape))\n",
    "# dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "# dense2 = dense(dense1, 32, activation=tf.tanh)\n",
    "# act_probs = dense(dense2, 2, activation=tf.nn.softmax)\n",
    "\n",
    "# v_dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "# v_dense2 = dense(v_dense1, 32, activation=tf.tanh)\n",
    "# value = dense(v_dense2, 1)\n",
    "\n",
    "# network = PPOTrainer(obs, act_probs, value, act_type='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = CartPoleEnv()\n",
    "# # env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "# obs = tf.placeholder(tf.float32, shape=[None]+list(env.observation_space.shape))\n",
    "# dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "# dense2 = dense(dense1, 32, activation=tf.tanh)\n",
    "# act_probs = dense(dense2, env.action_space.shape[0])\n",
    "\n",
    "# v_dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "# v_dense2 = dense(v_dense1, 32, activation=tf.tanh)\n",
    "# value = dense(v_dense2, 1)\n",
    "\n",
    "# network = PPOTrainer(obs, act_probs, value, act_type='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "obs = tf.placeholder(tf.float32, shape=[None]+list(env.observation_space.shape))\n",
    "dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "dense2 = dense(dense1, 32, activation=tf.tanh)\n",
    "act_probs = dense(dense2, env.action_space.shape[0], activation=tf.tanh)\n",
    "\n",
    "v_dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "v_dense2 = dense(v_dense1, 32, activation=tf.tanh)\n",
    "value = dense(v_dense2, 1)\n",
    "\n",
    "network = PPOTrainer(obs, act_probs, value, act_type='c', entropy_coef= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000000\n",
    "max_steps = 200\n",
    "update_freq = 16\n",
    "print_freq = 4\n",
    "\n",
    "mb = MemoryBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #4, Reward: -43.43938043580168\n",
      "[-0.4255836  -0.5417275   0.30097774  0.90512437]\n",
      "-1.143805\n",
      "\n",
      "Update #8, Reward: -19.216157109723902\n",
      "[ 0.9951526  -0.09123999  0.3803486   0.42581218]\n",
      "-1.1277367\n",
      "\n",
      "Update #12, Reward: -28.522169967619295\n",
      "[-0.870984    0.7447529   0.47475326 -0.19751024]\n",
      "-1.1302333\n",
      "\n",
      "Update #16, Reward: -38.8993607310927\n",
      "[-1.5737464  -0.20089933 -1.4391295  -1.3277129 ]\n",
      "-1.1358172\n",
      "\n",
      "Update #20, Reward: -76.92704975397344\n",
      "[-0.4982914   0.04332612 -0.13924053 -0.11839139]\n",
      "-1.1523395\n",
      "\n",
      "Update #24, Reward: -76.88723179886644\n",
      "[-0.60915834 -1.3590945   0.44240493  0.57904136]\n",
      "-1.1756155\n",
      "\n",
      "Update #28, Reward: -76.71226502661817\n",
      "[ 1.0316777   0.25532207  0.32458642 -1.2453634 ]\n",
      "-1.2102191\n",
      "\n",
      "Update #32, Reward: -65.05719565095742\n",
      "[-1.0413121   0.15279686  0.27340907  0.1524845 ]\n",
      "-1.209454\n",
      "\n",
      "Update #36, Reward: -25.08276429144662\n",
      "[-0.25311917 -0.2934596   1.1656295  -0.2686175 ]\n",
      "-1.1983747\n",
      "\n",
      "Update #40, Reward: -41.84326305195559\n",
      "[ 0.290689   -0.87865627  0.60204154  0.9393899 ]\n",
      "-1.2067349\n",
      "\n",
      "Update #44, Reward: -29.73879044607937\n",
      "[-0.7224914   1.1072545   0.29144162  0.7180375 ]\n",
      "-1.1712426\n",
      "\n",
      "Update #48, Reward: -22.034348825039075\n",
      "[-1.029614    0.01905119  0.15192448  0.83828735]\n",
      "-1.1686335\n",
      "\n",
      "Update #52, Reward: -29.207329989372802\n",
      "[-1.501678   0.8636875  0.7924528 -0.1091626]\n",
      "-1.1573231\n",
      "\n",
      "Update #56, Reward: -53.90193148307438\n",
      "[-0.5169493  -0.29417548 -0.49262884 -1.0984669 ]\n",
      "-1.1516844\n",
      "\n",
      "Update #60, Reward: -77.78036161596157\n",
      "[-2.0115314   0.49397    -0.65757096 -1.0957125 ]\n",
      "-1.1644219\n",
      "\n",
      "Update #64, Reward: -50.49335729784984\n",
      "[-1.5046537  0.5806228  1.216511  -1.425511 ]\n",
      "-1.1466682\n",
      "\n",
      "Update #68, Reward: -39.986762640912914\n",
      "[-0.8730407   0.8228704   0.11471161 -2.0582776 ]\n",
      "-1.151659\n",
      "\n",
      "Update #72, Reward: -39.63512427395486\n",
      "[-1.7570461  1.5292485  0.9090916 -0.482841 ]\n",
      "-1.1553152\n",
      "\n",
      "Update #76, Reward: -42.17750797865497\n",
      "[-0.43093836  0.43699753 -0.63325346 -2.0659013 ]\n",
      "-1.1631489\n",
      "\n",
      "Update #80, Reward: -44.861638406225964\n",
      "[-0.9837592  1.1577697  1.0807121 -1.5988095]\n",
      "-1.1436749\n",
      "\n",
      "Update #84, Reward: -30.732108184859495\n",
      "[-1.3457613   1.2220002   0.495448   -0.17545241]\n",
      "-1.1234066\n",
      "\n",
      "Update #88, Reward: -41.26078835636023\n",
      "[-0.3583659  0.3034467  1.259303  -0.7732668]\n",
      "-1.13534\n",
      "\n",
      "Update #92, Reward: -35.69961385503982\n",
      "[-1.0015887  0.5973502  0.8224569 -1.1849554]\n",
      "-1.1399536\n",
      "\n",
      "Update #96, Reward: -25.38952445735117\n",
      "[-0.94908994  1.1303985   1.8160021  -0.62478995]\n",
      "-1.1352665\n",
      "\n",
      "Update #100, Reward: -42.51283413466254\n",
      "[-1.4045701   0.45044053  1.2948896  -0.3086779 ]\n",
      "-1.1412987\n",
      "\n",
      "Update #104, Reward: -38.6836889383042\n",
      "[-1.5993235   0.14414746  0.3559491  -0.48875737]\n",
      "-1.1591964\n",
      "\n",
      "Update #108, Reward: -35.72553661769691\n",
      "[-0.5944061  1.3282478  0.5562825 -0.6132904]\n",
      "-1.1823854\n",
      "\n",
      "Update #112, Reward: -37.835806939051444\n",
      "[-0.8272674   0.31783757  1.5897766  -0.46525437]\n",
      "-1.1826122\n",
      "\n",
      "Update #116, Reward: -52.963299011275076\n",
      "[-0.49908668  0.7234906  -0.35866612 -1.2202054 ]\n",
      "-1.2081875\n",
      "\n",
      "Update #120, Reward: -45.930191838042575\n",
      "[-0.54011464  1.1385283  -0.865115   -2.1144748 ]\n",
      "-1.2354242\n",
      "\n",
      "Update #124, Reward: -53.91555292107421\n",
      "[-0.73272216  0.963999    0.07400262  0.08255315]\n",
      "-1.2447468\n",
      "\n",
      "Update #128, Reward: -37.003347434392694\n",
      "[-0.17881662  0.9287538   1.3802624  -1.0743343 ]\n",
      "-1.2275093\n",
      "\n",
      "Update #132, Reward: -42.42532350137711\n",
      "[-1.3184115   2.0327992   1.0402563  -0.41297764]\n",
      "-1.2356508\n",
      "\n",
      "Update #136, Reward: -32.302833375082216\n",
      "[-0.7867801  1.6446856  1.3685603 -2.44476  ]\n",
      "-1.2093874\n",
      "\n",
      "Update #140, Reward: -49.47348089060631\n",
      "[-1.4494305  1.3394928  2.8514118 -1.5017483]\n",
      "-1.217661\n",
      "\n",
      "Update #144, Reward: -39.893679441831296\n",
      "[-1.2736895  1.4910978  1.7571754 -1.2751575]\n",
      "-1.2361436\n",
      "\n",
      "Update #148, Reward: -36.004808407320105\n",
      "[-0.64559555  1.4818412   2.1419659  -1.5524807 ]\n",
      "-1.210145\n",
      "\n",
      "Update #152, Reward: -39.89078763342718\n",
      "[-1.2047485   3.0396233  -0.2133295  -0.60380447]\n",
      "-1.2165298\n",
      "\n",
      "Update #156, Reward: -41.26788856614715\n",
      "[-2.0767443  0.8178907  1.5342293 -0.4308107]\n",
      "-1.2342561\n",
      "\n",
      "Update #160, Reward: -45.0689140612582\n",
      "[-1.0041409   0.19777864  0.6233498  -1.0184343 ]\n",
      "-1.2589842\n",
      "\n",
      "Update #164, Reward: -41.48334042972856\n",
      "[-1.3694454   0.00485933  0.76570755 -0.6059102 ]\n",
      "-1.2707152\n",
      "\n",
      "Update #168, Reward: -50.661159494248054\n",
      "[ 0.19436514  0.92919797  0.24720728 -0.9635243 ]\n",
      "-1.2873678\n",
      "\n",
      "Update #172, Reward: -48.731259845695135\n",
      "[-0.9590925  0.8618611  0.3955199 -1.3888764]\n",
      "-1.255717\n",
      "\n",
      "Update #176, Reward: -57.400135056754095\n",
      "[-0.10440958 -0.12172633  1.7405016  -0.65075743]\n",
      "-1.2705047\n",
      "\n",
      "Update #180, Reward: -48.15838955116672\n",
      "[-0.46567822  0.9098139   1.5010266  -0.84364533]\n",
      "-1.2746673\n",
      "\n",
      "Update #184, Reward: -40.222131168844086\n",
      "[-0.01200783  1.5241046   0.3501156  -1.3632396 ]\n",
      "-1.2430559\n",
      "\n",
      "Update #188, Reward: -53.22320088460866\n",
      "[-0.02108651  2.1492612   0.58392143 -1.2125614 ]\n",
      "-1.2480263\n",
      "\n",
      "Update #192, Reward: -39.304238431380796\n",
      "[-1.5603495  0.8750632  1.3497584 -1.638519 ]\n",
      "-1.2617097\n",
      "\n",
      "Update #196, Reward: -45.9947666638368\n",
      "[ 0.11876667  0.87744576 -0.26683515 -1.1911716 ]\n",
      "-1.261983\n",
      "\n",
      "Update #200, Reward: -51.80197456790265\n",
      "[-0.860907   1.2413833  1.4526038 -0.897194 ]\n",
      "-1.2789145\n",
      "\n",
      "Update #204, Reward: -43.93754993643155\n",
      "[ 0.00215304  1.0405908   0.0125792  -1.8847823 ]\n",
      "-1.2735645\n",
      "\n",
      "Update #208, Reward: -54.1799394712549\n",
      "[-0.13184214  0.31790125  0.95737594 -0.3216654 ]\n",
      "-1.2767401\n",
      "\n",
      "Update #212, Reward: -50.839967825328486\n",
      "[-1.6411276   2.4761302   0.14720178 -0.6036741 ]\n",
      "-1.274363\n",
      "\n",
      "Update #216, Reward: -50.5065632118171\n",
      "[-1.6752508   0.4930911   0.99274784  0.5222163 ]\n",
      "-1.3011312\n",
      "\n",
      "Update #220, Reward: -42.185715684997454\n",
      "[-0.5908381   0.7015304   0.10206866 -0.27276272]\n",
      "-1.3037026\n",
      "\n",
      "Update #224, Reward: -49.48127315932683\n",
      "[-0.8200138  -1.6542203   0.67533815 -1.7860432 ]\n",
      "-1.2986192\n",
      "\n",
      "Update #228, Reward: -53.509801952810356\n",
      "[-1.9693336  1.0496877  1.866014  -2.6903825]\n",
      "-1.2998127\n",
      "\n",
      "Update #232, Reward: -43.41535179847715\n",
      "[-0.34346557  1.3301706   0.35483798 -0.94249713]\n",
      "-1.2631329\n",
      "\n",
      "Update #236, Reward: -36.19830300925314\n",
      "[-1.4108264  0.7589366  0.5947226 -2.7906022]\n",
      "-1.2627656\n",
      "\n",
      "Update #240, Reward: -46.68467255520251\n",
      "[-1.0916321   2.832635    0.73956096 -0.8970298 ]\n",
      "-1.2532141\n",
      "\n",
      "Update #244, Reward: -42.0210456391777\n",
      "[-2.3343713   1.964211    0.38324082 -2.0762057 ]\n",
      "-1.2312981\n",
      "\n",
      "Update #248, Reward: -42.19260090357515\n",
      "[-0.24337661  1.7311082   0.45291436 -2.0017297 ]\n",
      "-1.2412221\n",
      "\n",
      "Update #252, Reward: -49.6873195538354\n",
      "[-2.2999914   1.8651979   1.7157333   0.47412324]\n",
      "-1.186294\n",
      "\n",
      "Update #256, Reward: -44.924920061826\n",
      "[-0.8569894  2.281508   0.7506779 -0.7520891]\n",
      "-1.1867595\n",
      "\n",
      "Update #260, Reward: -38.77226504691595\n",
      "[-0.6304435   1.0451071   0.7822318   0.41215992]\n",
      "-1.1877389\n",
      "\n",
      "Update #264, Reward: -37.548573698685665\n",
      "[-2.2530494  -0.06172824  1.3133566  -1.4955609 ]\n",
      "-1.1908238\n",
      "\n",
      "Update #268, Reward: -48.9184880863405\n",
      "[-0.4822783  1.1872575  2.2941465 -0.3819427]\n",
      "-1.2050631\n",
      "\n",
      "Update #272, Reward: -49.37421739657292\n",
      "[-0.51775026  0.9119156  -0.8196385  -0.80779797]\n",
      "-1.1576303\n",
      "\n",
      "Update #276, Reward: -40.521496255050295\n",
      "[-0.79155743  0.99161106  1.9748775  -0.49452603]\n",
      "-1.090561\n",
      "\n",
      "Update #280, Reward: -42.93186987036867\n",
      "[-0.10951811  0.7394201   0.7168538   0.39292195]\n",
      "-1.108226\n",
      "\n",
      "Update #284, Reward: -43.154368766443\n",
      "[-1.5488237   0.16108626  0.55807906 -0.7275333 ]\n",
      "-1.0780783\n",
      "\n",
      "Update #288, Reward: -40.5992450123828\n",
      "[-0.67191136  0.59471965  0.49102736 -0.6384864 ]\n",
      "-1.0859461\n",
      "\n",
      "Update #292, Reward: -45.001715359274286\n",
      "[-0.5354765   0.6534561   0.48986748 -0.8737105 ]\n",
      "-1.0432467\n",
      "\n",
      "Update #296, Reward: -41.297083220359575\n",
      "[-0.09981495  1.0601476   1.4835958  -0.6431841 ]\n",
      "-1.0547055\n",
      "\n",
      "Update #300, Reward: -40.48355445062646\n",
      "[-0.72956055  0.82304853  1.2559322  -1.0238339 ]\n",
      "-1.0437803\n",
      "\n",
      "Update #304, Reward: -38.622670297410906\n",
      "[-1.2481914  1.1360674  0.8381167 -0.5458984]\n",
      "-0.9663117\n",
      "\n",
      "Update #308, Reward: -39.81433736976667\n",
      "[-1.0735632  1.5204186  1.0074064 -1.4397452]\n",
      "-0.98648465\n",
      "\n",
      "Update #312, Reward: -38.326222834758326\n",
      "[-1.5773035  1.4610808  0.7113464 -1.5340556]\n",
      "-0.9513005\n",
      "\n",
      "Update #316, Reward: -43.6755327624206\n",
      "[-0.88033116  0.9212884   0.12160838 -1.1608949 ]\n",
      "-0.9092302\n",
      "\n",
      "Update #320, Reward: -35.31686773911576\n",
      "[-0.04465446  0.8085698   0.58901894  0.13793097]\n",
      "-0.94843405\n",
      "\n",
      "Update #324, Reward: -33.70350885496191\n",
      "[-1.0878556   0.91851956  0.31782517 -0.65276825]\n",
      "-0.95004255\n",
      "\n",
      "Update #328, Reward: -32.400876113945515\n",
      "[-0.8665001   0.6864475   0.5641978  -0.84653324]\n",
      "-0.9615791\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #332, Reward: -39.69723026910215\n",
      "[-0.635688   0.9715442  1.319759  -1.1028706]\n",
      "-0.98227537\n",
      "\n",
      "Update #336, Reward: -38.2094051750145\n",
      "[-0.8268071  1.120253   1.5296865 -0.9812632]\n",
      "-0.8993496\n",
      "\n",
      "Update #340, Reward: -32.8101100571488\n",
      "[-1.3112996   0.34696788  1.469564   -0.8463721 ]\n",
      "-0.8687394\n",
      "\n",
      "Update #344, Reward: -36.11554465579997\n",
      "[-0.80090475  0.85038245  0.5737298  -1.044363  ]\n",
      "-0.842275\n",
      "\n",
      "Update #348, Reward: -29.293584867583924\n",
      "[-1.1665761  0.6547328  1.2842175 -0.917302 ]\n",
      "-0.8172947\n",
      "\n",
      "Update #352, Reward: -25.07329644443478\n",
      "[-0.758932   0.4005286  1.0086255 -1.111101 ]\n",
      "-0.7864876\n",
      "\n",
      "Update #356, Reward: -36.31051630171578\n",
      "[-0.62096405  0.9888075   1.1354481  -1.011633  ]\n",
      "-0.75763166\n",
      "\n",
      "Update #360, Reward: -28.148349136669967\n",
      "[-1.1937474   1.2579644   0.55164766 -1.1891911 ]\n",
      "-0.64112353\n",
      "\n",
      "Update #364, Reward: -23.990607219165604\n",
      "[-0.7601409   0.62794894  1.1065482  -1.0461428 ]\n",
      "-0.61114883\n",
      "\n",
      "Update #368, Reward: -21.47941712962483\n",
      "[-0.9740763  0.8893909  0.9211183 -0.7676903]\n",
      "-0.6347925\n",
      "\n",
      "Update #372, Reward: -25.748490355524645\n",
      "[-1.0136461   0.9060879   1.6892524  -0.47281182]\n",
      "-0.60589236\n",
      "\n",
      "Update #376, Reward: -22.862159375212954\n",
      "[-1.1570204   1.0869141   0.97295254 -0.7820168 ]\n",
      "-0.5792102\n",
      "\n",
      "Update #380, Reward: -22.843538256656192\n",
      "[-0.88208306  1.2829611   0.8411778  -1.3352408 ]\n",
      "-0.6043412\n",
      "\n",
      "Update #384, Reward: -18.72197125360623\n",
      "[-0.78600335  0.88660306  0.91930807 -0.5185859 ]\n",
      "-0.5726317\n",
      "\n",
      "Update #388, Reward: -24.13275205204909\n",
      "[-0.8874125  0.863855   1.1833462 -1.5705361]\n",
      "-0.60049987\n",
      "\n",
      "Update #392, Reward: -21.515423456465417\n",
      "[-0.86986506  1.0670323   0.8260311  -0.9649149 ]\n",
      "-0.571444\n",
      "\n",
      "Update #396, Reward: -22.799348907290195\n",
      "[-1.0733988   0.99370146  1.3406403  -0.9770532 ]\n",
      "-0.5990733\n",
      "\n",
      "Update #400, Reward: -29.84461334993933\n",
      "[-0.97818327  1.0550857   0.81446135 -1.437821  ]\n",
      "-0.6264877\n",
      "\n",
      "Update #404, Reward: -24.23004056109147\n",
      "[-1.0805327  0.9899228  1.4231083 -1.1380594]\n",
      "-0.54647005\n",
      "\n",
      "Update #408, Reward: -24.414585020117986\n",
      "[-0.9680659   1.15073     0.99196154 -0.91110045]\n",
      "-0.5231994\n",
      "\n",
      "Update #412, Reward: -24.5232094443652\n",
      "[-0.9806906   0.9295109   0.9354947  -0.75834244]\n",
      "-0.55523777\n",
      "\n",
      "Update #416, Reward: -21.621263588111933\n",
      "[-1.1353453   0.9749568   0.81883764 -0.97953075]\n",
      "-0.5284187\n",
      "\n",
      "Update #420, Reward: -25.81715644240608\n",
      "[-0.5071481   0.92845184  0.8547697  -1.5179439 ]\n",
      "-0.55758053\n",
      "\n",
      "Update #424, Reward: -24.60497321970465\n",
      "[-1.1080884  1.0720824  1.0879062 -1.0684345]\n",
      "-0.537083\n",
      "\n",
      "Update #428, Reward: -25.873186016450184\n",
      "[-1.3613015   0.87183857  0.855965   -0.96793735]\n",
      "-0.498968\n",
      "\n",
      "Update #432, Reward: -20.306173248022432\n",
      "[-1.0713781  1.0512992  1.1855499 -0.9541364]\n",
      "-0.43830046\n",
      "\n",
      "Update #436, Reward: -23.25963069937314\n",
      "[-1.105938    1.0493659   0.82201385 -1.257345  ]\n",
      "-0.4126921\n",
      "\n",
      "Update #440, Reward: -19.10689231732359\n",
      "[-0.78232205  0.98218     0.889838   -1.3546591 ]\n",
      "-0.43370032\n",
      "\n",
      "Update #444, Reward: -20.383133101203917\n",
      "[-0.97071457  1.0218242   1.0375584  -0.56954706]\n",
      "-0.40786937\n",
      "\n",
      "Update #448, Reward: -19.141161029229465\n",
      "[-0.978517    1.1053997   1.1967998  -0.83208907]\n",
      "-0.37827387\n",
      "\n",
      "Update #452, Reward: -21.914361969108164\n",
      "[-1.1664649  1.0648988  1.3498285 -1.0120242]\n",
      "-0.4143906\n",
      "\n",
      "Update #456, Reward: -19.117572561606043\n",
      "[-0.8258519   0.88210267  0.9170674  -1.0222894 ]\n",
      "-0.33410016\n",
      "\n",
      "Update #460, Reward: -20.55942453225973\n",
      "[-0.9729207   1.1661621   0.95835286 -0.92295647]\n",
      "-0.35972685\n",
      "\n",
      "Update #464, Reward: -19.149193749254096\n",
      "[-1.0199122  1.0660627  1.10811   -0.9551342]\n",
      "-0.3132314\n",
      "\n",
      "Update #468, Reward: -19.308570719147987\n",
      "[-0.96259797  0.84524614  1.2487832  -0.8393074 ]\n",
      "-0.33839566\n",
      "\n",
      "Update #472, Reward: -19.161103824539605\n",
      "[-0.80688953  1.0695691   1.1986499  -0.95237565]\n",
      "-0.3609042\n",
      "\n",
      "Update #476, Reward: -20.452900897385433\n",
      "[-1.0096347  1.109525   0.8609111 -0.9878542]\n",
      "-0.33196378\n",
      "\n",
      "Update #480, Reward: -21.842834521257195\n",
      "[-0.8927907   0.9636406   0.90769917 -1.1461236 ]\n",
      "-0.36278665\n",
      "\n",
      "Update #484, Reward: -20.464403403442176\n",
      "[-1.1385225   0.9669804   1.304207   -0.93845594]\n",
      "-0.38676327\n",
      "\n",
      "Update #488, Reward: -19.129352880146854\n",
      "[-0.99016005  1.0731361   0.93667084 -0.99883944]\n",
      "-0.40655422\n",
      "\n",
      "Update #492, Reward: -19.111709676095337\n",
      "[-0.8506003  1.1476269  1.2903925 -0.9801275]\n",
      "-0.33086604\n",
      "\n",
      "Update #496, Reward: -19.18896347006222\n",
      "[-1.0240985   0.98114026  1.1374533  -1.0679691 ]\n",
      "-0.36235172\n",
      "\n",
      "Update #500, Reward: -19.173395395754426\n",
      "[-1.0025058   0.8635032   1.387218   -0.93152165]\n",
      "-0.3908404\n",
      "\n",
      "Update #504, Reward: -21.969532855650648\n",
      "[-1.0161784  1.1214492  1.0265    -0.9287387]\n",
      "-0.4245525\n",
      "\n",
      "Update #508, Reward: -19.165387609987032\n",
      "[-0.84067184  1.1017588   0.7801802  -1.207312  ]\n",
      "-0.39484647\n",
      "\n",
      "Update #512, Reward: -21.98347359634409\n",
      "[-0.8071531  1.0133015  1.0829902 -1.268593 ]\n",
      "-0.41970378\n",
      "\n",
      "Update #516, Reward: -20.452006915303404\n",
      "[-1.1145722  0.9339235  1.2418563 -0.9965763]\n",
      "-0.391137\n",
      "\n",
      "Update #520, Reward: -20.382995202781032\n",
      "[-0.8099412  0.7943824  1.1277422 -1.1227365]\n",
      "-0.36433664\n",
      "\n",
      "Update #524, Reward: -21.935643848290876\n",
      "[-1.0206523  0.9565923  1.2519855 -1.2066566]\n",
      "-0.39251217\n",
      "\n",
      "Update #528, Reward: -21.962339962213584\n",
      "[-0.97969556  0.9950579   1.1233051  -0.9811987 ]\n",
      "-0.37143254\n",
      "\n",
      "Update #532, Reward: -22.05599631103548\n",
      "[-0.95577693  1.0361291   1.2459795  -1.1495073 ]\n",
      "-0.34388584\n",
      "\n",
      "Update #536, Reward: -19.275402973983905\n",
      "[-1.0465024  1.1652987  0.9701002 -1.1010723]\n",
      "-0.3161054\n",
      "\n",
      "Update #540, Reward: -18.998189695400647\n",
      "[-0.9948106   0.91508144  1.1370776  -1.0049484 ]\n",
      "-0.18609264\n",
      "\n",
      "Update #544, Reward: -18.72386888732788\n",
      "[-1.014401   0.9231552  0.9887383 -1.0877533]\n",
      "-0.1755358\n",
      "\n",
      "Update #548, Reward: -19.56976134194723\n",
      "[-0.99877024  0.96147233  0.7303436  -0.9599397 ]\n",
      "-0.2058648\n",
      "\n",
      "Update #552, Reward: -19.452990572830792\n",
      "[-1.0028007   0.90159076  1.1363782  -0.8663483 ]\n",
      "-0.2371639\n",
      "\n",
      "Update #556, Reward: -19.52870920774421\n",
      "[-1.0779952   1.0662612   1.1580124  -0.82469404]\n",
      "-0.2630014\n",
      "\n",
      "Update #560, Reward: -19.411463348252603\n",
      "[-0.98136353  0.9687821   0.9204356  -1.0000675 ]\n",
      "-0.23612331\n",
      "\n",
      "Update #564, Reward: -19.450629282637802\n",
      "[-0.94926965  1.0096368   0.8987663  -0.96894747]\n",
      "-0.26018852\n",
      "\n",
      "Update #568, Reward: -19.38294213402657\n",
      "[-0.98231035  0.9598682   1.0936254  -0.9798268 ]\n",
      "-0.28447166\n",
      "\n",
      "Update #572, Reward: -19.387238076717168\n",
      "[-1.1437668  0.9142897  1.1111854 -0.8874327]\n",
      "-0.21378878\n",
      "\n",
      "Update #576, Reward: -19.504789346929222\n",
      "[-0.94785166  0.9777304   0.9087762  -1.0314198 ]\n",
      "-0.14290263\n",
      "\n",
      "Update #580, Reward: -19.464193492243727\n",
      "[-0.93474615  1.0864756   0.9260521  -1.0333444 ]\n",
      "-0.07111704\n",
      "\n",
      "Update #584, Reward: -22.44992753856711\n",
      "[-0.93927073  0.9609721   0.9660763  -1.0284034 ]\n",
      "-0.04317312\n",
      "\n",
      "Update #588, Reward: -19.675177207278185\n",
      "[-1.0074682  1.0466324  0.9773791 -0.8741893]\n",
      "-0.08313595\n",
      "\n",
      "Update #592, Reward: -19.773053118432223\n",
      "[-0.99349034  0.9752104   0.9678883  -0.8797401 ]\n",
      "-0.06766779\n",
      "\n",
      "Update #596, Reward: -19.854124935286173\n",
      "[-1.0193129  0.9563633  0.9953257 -1.2753814]\n",
      "-0.1018209\n",
      "\n",
      "Update #600, Reward: -19.833542113910624\n",
      "[-1.0276573  0.9526516  1.1012148 -1.0321753]\n",
      "-0.13574202\n",
      "\n",
      "Update #604, Reward: -19.740952520289532\n",
      "[-1.0809585   1.0121093   1.041568   -0.84051794]\n",
      "-0.16185457\n",
      "\n",
      "Update #608, Reward: -19.74290427511192\n",
      "[-1.0263457  1.0146883  0.8550512 -0.9380675]\n",
      "-0.1360451\n",
      "\n",
      "Update #612, Reward: -19.717219106843366\n",
      "[-0.95458287  1.0020407   1.0648403  -0.8629601 ]\n",
      "-0.16822612\n",
      "\n",
      "Update #616, Reward: -19.686418152094102\n",
      "[-0.99728304  1.029098    0.95555085 -0.9938152 ]\n",
      "-0.19532917\n",
      "\n",
      "Update #620, Reward: -19.634713035081496\n",
      "[-1.0071684   0.9394463   0.97966826 -1.1392946 ]\n",
      "-0.16708714\n",
      "\n",
      "Update #624, Reward: -19.71389899558242\n",
      "[-1.0770727   1.0468858   1.0281166  -0.99474806]\n",
      "-0.14695339\n",
      "\n",
      "Update #628, Reward: -19.76775610951809\n",
      "[-0.95846474  0.9761131   1.0521438  -1.177539  ]\n",
      "-0.034205414\n",
      "\n",
      "Update #632, Reward: -19.779253679367237\n",
      "[-0.9639039  1.0125961  0.9556325 -1.0172987]\n",
      "-0.0105398\n",
      "\n",
      "Update #636, Reward: -19.772528823284887\n",
      "[-0.98449785  0.98164064  1.0808572  -1.0648117 ]\n",
      "0.010458589\n",
      "\n",
      "Update #640, Reward: -19.4294005332194\n",
      "[-0.98956597  1.0062162   1.0805991  -0.98424196]\n",
      "0.018114522\n",
      "\n",
      "Update #644, Reward: -19.537900054001778\n",
      "[-0.98670137  1.0246222   0.9665976  -0.93446696]\n",
      "0.036539912\n",
      "\n",
      "Update #648, Reward: -19.693030879802762\n",
      "[-1.0140253  0.9875335  0.9059682 -1.1248499]\n",
      "0.09593603\n",
      "\n",
      "Update #652, Reward: -19.61224851584133\n",
      "[-0.9998865  1.002915   1.0862241 -0.9536053]\n",
      "0.10713263\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #656, Reward: -19.781262063211813\n",
      "[-1.007101    0.9455691   0.9929968  -0.85025483]\n",
      "0.12674642\n",
      "\n",
      "Update #660, Reward: -19.574309165507113\n",
      "[-0.9829064  0.9831375  1.0195187 -1.1171627]\n",
      "0.14429712\n",
      "\n",
      "Update #664, Reward: -19.633779329566863\n",
      "[-0.9754566   1.0227672   0.9125076  -0.95613253]\n",
      "0.16477662\n",
      "\n",
      "Update #668, Reward: -19.781675769953765\n",
      "[-1.0008862   1.0136127   1.0215783  -0.92012054]\n",
      "0.17876133\n",
      "\n",
      "Update #672, Reward: -19.737308259461834\n",
      "[-1.0238682   1.0211111   0.96695036 -1.0611358 ]\n",
      "0.19279085\n",
      "\n",
      "Update #676, Reward: -19.699350631717873\n",
      "[-0.96512765  1.0586448   0.927963   -1.0317165 ]\n",
      "0.16528895\n",
      "\n",
      "Update #680, Reward: -19.662520158055443\n",
      "[-1.0124571   1.0058789   1.0352784  -0.96609783]\n",
      "0.19005863\n",
      "\n",
      "Update #684, Reward: -19.72233126094994\n",
      "[-1.0013945  1.0215665  1.0040346 -1.0608859]\n",
      "0.21379054\n",
      "\n",
      "Update #688, Reward: -19.616905251829312\n",
      "[-1.0075969   0.98703486  1.0174781  -1.0275595 ]\n",
      "0.22563228\n",
      "\n",
      "Update #692, Reward: -19.65186852603746\n",
      "[-1.0133933   1.0167439   0.97778165 -1.0163195 ]\n",
      "0.19139007\n",
      "\n",
      "Update #696, Reward: -19.934239814469425\n",
      "[-1.0073884   0.96427894  1.043373   -0.99607235]\n",
      "0.1930748\n",
      "\n",
      "Update #700, Reward: -41.671034138399975\n",
      "[-1.0233312  0.9900824  1.0188214 -1.1056983]\n",
      "0.35853916\n",
      "\n",
      "Update #704, Reward: -20.155386696446477\n",
      "[-0.9860758   0.99402964  1.0577426  -0.87216645]\n",
      "0.3853513\n",
      "\n",
      "Update #708, Reward: -20.178099147421257\n",
      "[-1.0272061   0.99371386  0.9519613  -0.9431402 ]\n",
      "0.45417246\n",
      "\n",
      "Update #712, Reward: -20.1630575691335\n",
      "[-0.99476504  1.0116909   0.86408967 -0.97445226]\n",
      "0.47324532\n",
      "\n",
      "Update #716, Reward: -20.18294004896701\n",
      "[-0.9944553   0.99817055  1.0060585  -0.9264052 ]\n",
      "0.48823878\n",
      "\n",
      "Update #720, Reward: -20.135994244406596\n",
      "[-0.9998099  1.0025524  1.0322272 -0.8749931]\n",
      "0.44667447\n",
      "\n",
      "Update #724, Reward: -20.079863634936558\n",
      "[-1.0126044   0.99393755  0.97371936 -1.099687  ]\n",
      "0.46435776\n",
      "\n",
      "Update #728, Reward: -20.158978839582165\n",
      "[-0.99018943  0.9994589   1.056157   -0.95635843]\n",
      "0.4730105\n",
      "\n",
      "Update #732, Reward: -20.10170749941708\n",
      "[-0.9906248  0.9881973  1.0424021 -1.0770866]\n",
      "0.43548185\n",
      "\n",
      "Update #736, Reward: -20.144750660435218\n",
      "[-0.9945229  1.010713   0.984633  -1.0046879]\n",
      "0.40433013\n",
      "\n",
      "Update #740, Reward: -20.098091431082626\n",
      "[-1.0055572   0.9980317   0.95406073 -1.0272391 ]\n",
      "0.40749004\n",
      "\n",
      "Update #744, Reward: -20.04474153961047\n",
      "[-1.0186958   1.0176874   1.0672715  -0.98548496]\n",
      "0.35934666\n",
      "\n",
      "Update #748, Reward: -20.061020890882624\n",
      "[-1.0075257  0.9966717  1.026197  -1.1025801]\n",
      "0.38375238\n",
      "\n",
      "Update #752, Reward: -19.920849052539097\n",
      "[-0.9972929  1.0078601  0.9102514 -0.9336251]\n",
      "0.40640083\n",
      "\n",
      "Update #756, Reward: -20.0605934171019\n",
      "[-0.9938935   0.99528426  0.9484999  -0.9669053 ]\n",
      "0.37066808\n",
      "\n",
      "Update #760, Reward: -20.001636727201042\n",
      "[-0.9934619  0.994051   1.0144814 -1.0548103]\n",
      "0.39439178\n",
      "\n",
      "Update #764, Reward: -20.025226881406923\n",
      "[-1.0104785  1.0009724  1.0867022 -1.1085352]\n",
      "0.35697785\n",
      "\n",
      "Update #768, Reward: -19.987688211822956\n",
      "[-1.007417   0.9924118  0.9600792 -0.9358983]\n",
      "0.33682156\n",
      "\n",
      "Update #772, Reward: -19.98374006121896\n",
      "[-0.9885715  0.9942134  0.9290296 -0.9465046]\n",
      "0.35160136\n",
      "\n",
      "Update #776, Reward: -20.063381978494235\n",
      "[-1.0037179   0.9790319   0.80678546 -0.8471423 ]\n",
      "0.32209703\n",
      "\n",
      "Update #780, Reward: -19.911477897262664\n",
      "[-1.0162677  1.0072955  1.1159201 -1.0910195]\n",
      "0.34925857\n",
      "\n",
      "Update #784, Reward: -19.982857581063357\n",
      "[-1.0120274  1.0007856  1.0399437 -1.1527916]\n",
      "0.37548906\n",
      "\n",
      "Update #788, Reward: -19.910616648464774\n",
      "[-0.9927077   0.9746887   0.90915805 -0.9290985 ]\n",
      "0.3930959\n",
      "\n",
      "Update #792, Reward: -19.96258677008903\n",
      "[-1.0077337  1.0148383  0.927748  -1.1355654]\n",
      "0.36123684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    ep_reward = 0\n",
    "    \n",
    "    mb.start_rollout()\n",
    "    obs = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        obs = obs.squeeze()\n",
    "        act = network.gen_act(obs)\n",
    "        \n",
    "        obs = obs / 5.\n",
    "        a_o = act\n",
    "        act = np.clip(act, -1, 1)\n",
    "        for i in obs:\n",
    "            if abs(i) > 1:\n",
    "                print('----------')\n",
    "                print(i)\n",
    "        \n",
    "        obs_next, rew, d, _ = env.step(act)\n",
    "        ep_reward += rew\n",
    "        \n",
    "        if False:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        mb.record(obs, act, rew)\n",
    "        obs = obs_next\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "            \n",
    "    all_rewards.append(ep_reward)\n",
    "            \n",
    "    if episode % update_freq == 0 and episode != 0:\n",
    "        kl_div, ent = network.train(mb.to_data())\n",
    "        \n",
    "        if episode % (update_freq * print_freq) == 0:\n",
    "            print(f'Update #{episode // update_freq}, Reward: {np.mean(all_rewards[-update_freq*print_freq:])}')\n",
    "            print(a_o)\n",
    "            print(ent)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

