{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cart_pole():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    new_rewards = [rewards[-1]]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(rewards[i] + gamma * new_rewards[-1])\n",
    "    return new_rewards[::-1]\n",
    "\n",
    "class MemoryBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1\n",
    "    \n",
    "    def start_rollout(self):\n",
    "        self.rollout_idx = (self.rollout_idx + 1) % self.max_size\n",
    "        if self.rollout_idx >= len(self.rollouts):\n",
    "            self.rollouts.append([])\n",
    "        else:\n",
    "            self.rollouts[self.rollout_idx] = []\n",
    "            \n",
    "    def end_rollout(self):\n",
    "        self.start_rollout()\n",
    "    \n",
    "    def record(self, obs, act, rew):\n",
    "        self.rollouts[self.rollout_idx].append([obs, act, rew])\n",
    "        \n",
    "    def to_data(self, reset=True):\n",
    "        all_data = []\n",
    "        \n",
    "        try:\n",
    "            for rollout in self.rollouts:\n",
    "                rollout = np.array(rollout)\n",
    "                # Discount the rewards for every rollout\n",
    "                rollout[:,2] = discount_rewards(rollout[:,2])\n",
    "                all_data.extend(list(rollout))\n",
    "\n",
    "            if reset:\n",
    "                self.reset()\n",
    "        except IndexError:\n",
    "            return np.array([])\n",
    "            \n",
    "        return np.array(all_data)\n",
    "                \n",
    "    def reset(self):\n",
    "        self.rollouts = []\n",
    "        self.rollout_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, obs_shape, act_space, sess=None):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.act_space = act_space\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.build_network()\n",
    "        \n",
    "    def build_network(self):\n",
    "        self.obs = tf.placeholder(tf.float64, shape=[None, self.obs_shape])\n",
    "        self.dense1 = dense(self.obs, 128, activation=tf.tanh)\n",
    "        self.dense2 = dense(self.dense1, 128, activation=tf.tanh)\n",
    "        self.act_probs = dense(self.dense2, self.act_space)\n",
    "        self.softmax_probs = tf.nn.softmax(self.act_probs)\n",
    "        self.chosen_act = tf.squeeze(tf.multinomial(logits=self.softmax_probs, num_samples=1), axis=1)\n",
    "        self.actions = tf.placeholder(tf.int64, shape=[None])\n",
    "        self.rewards = tf.placeholder(tf.float64, shape=[None])\n",
    "        \n",
    "        self.action_masks = tf.one_hot(self.actions, self.act_space, dtype=tf.float64)\n",
    "        self.log_probs = tf.log(self.softmax_probs)\n",
    "        \n",
    "        self.resp_actions = tf.reduce_sum(self.action_masks *  self.log_probs, axis=1)\n",
    "        self.loss = -tf.reduce_mean(self.resp_actions * self.rewards)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "#         self.action_masks = tf.one_hot(self.actions, self.act_space, dtype=tf.float64)\n",
    "#         self.resp_actions = tf.reduce_sum(self.action_masks * tf.log(self.out), axis=1)\n",
    "        \n",
    "#         self.loss = -tf.reduce_mean(self.resp_actions * self.rewards)\n",
    "        \n",
    "# #         self.loss = -tf.reduce_mean(tf.transpose(tf.log(self.resp_actions)) * self.rewards)\n",
    "#         self.optimizer = tf.train.AdamOptimizer()\n",
    "#         self.optimize = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.__enter__()\n",
    "        return self.sess\n",
    "    \n",
    "    def end_session(self):\n",
    "        if self.sess is not None:\n",
    "            self.sess.close()\n",
    "        \n",
    "    def choose_action1(self, obs, sess=None):\n",
    "        if not sess:\n",
    "            if self.sess is not None:\n",
    "                sess = self.sess\n",
    "            else:\n",
    "                sess = tf.get_default_session()\n",
    "                assert sess is not None\n",
    "        \n",
    "        act = sess.run(self.chosen_act, feed_dict={self.obs: obs})\n",
    "        return act\n",
    "    \n",
    "    def choose_action2(self, obs, sess=None):\n",
    "        if not sess:\n",
    "            if self.sess is not None:\n",
    "                sess = self.sess\n",
    "            else:\n",
    "                sess = tf.get_default_session()\n",
    "                assert sess is not None\n",
    "        \n",
    "        probs = sess.run(self.softmax_probs, feed_dict={self.obs: obs})\n",
    "        act = np.random.choice(list(range(len(probs)+1)), p=probs[0])\n",
    "        \n",
    "        return act, probs\n",
    "    \n",
    "    def train(self, train_data, sess=None):\n",
    "        \"\"\"train_data: in the two-dimensional numpy array with the format, [obs, act, rew]\"\"\"\n",
    "        if not sess:\n",
    "            if self.sess is not None:\n",
    "                sess = self.sess\n",
    "            else:\n",
    "                sess = tf.get_default_session()\n",
    "                assert sess is not None\n",
    "        \n",
    "        a, b, _ = sess.run([self.softmax_probs, tf.multinomial(logits=self.softmax_probs,num_samples=1), self.update], feed_dict={self.obs: np.vstack(train_data[:,0]),\n",
    "                                            self.actions: train_data[:,1],\n",
    "                                            self.rewards: train_data[:,2]})\n",
    "#         print(a[:50])\n",
    "#         print(b[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "max_steps = 200\n",
    "update_freq = 200 # In episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MemoryBuffer(update_freq)\n",
    "env = make_cart_pole()\n",
    "network = Network(env.observation_space.shape[0], env.action_space.n)\n",
    "network.init_session();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50038762 0.49961238]] 1 1\n",
      "[[0.52890111 0.47109889]] 1 0\n",
      "[[0.54616041 0.45383959]] 1 0\n",
      "[[0.50775307 0.49224693]] 0 1\n",
      "Recent Reward: 19.78\n",
      "Total Episodes: 100\n",
      "Total Steps: 1978\n",
      "\n",
      "-----------\n",
      "[[0.48490568 0.51509432]] 1 1\n",
      "[[0.48429321 0.51570679]] 1 1\n",
      "[[0.48589296 0.51410704]] 0 1\n",
      "[[0.49357878 0.50642122]] 1 0\n",
      "[[0.48060973 0.51939027]] 0 0\n",
      "Recent Reward: 22.74\n",
      "Total Episodes: 200\n",
      "Total Steps: 4252\n",
      "\n",
      "-----------\n",
      "[[0.65127712 0.34872288]] 0 0\n",
      "[[0.60214459 0.39785541]] 0 1\n",
      "[[0.50874221 0.49125779]] 0 0\n",
      "[[0.52476519 0.47523481]] 0 0\n",
      "[[0.36807044 0.63192956]] 0 0\n",
      "Recent Reward: 27.23\n",
      "Total Episodes: 300\n",
      "Total Steps: 6975\n",
      "\n",
      "-----------\n",
      "[[0.60068386 0.39931614]] 0 0\n",
      "[[0.55868244 0.44131756]] 0 0\n",
      "[[0.4882034 0.5117966]] 1 0\n",
      "[[0.25701717 0.74298283]] 1 1\n",
      "[[0.44029533 0.55970467]] 0 0\n",
      "[[0.45022957 0.54977043]] 1 1\n",
      "[[0.56965072 0.43034928]] 1 1\n",
      "Recent Reward: 34.58\n",
      "Total Episodes: 400\n",
      "Total Steps: 10433\n",
      "\n",
      "-----------\n",
      "[[0.46080438 0.53919562]] 1 0\n",
      "[[0.49342774 0.50657226]] 0 1\n",
      "[[0.58354318 0.41645682]] 1 0\n",
      "[[0.4402487 0.5597513]] 0 0\n",
      "[[0.38803192 0.61196808]] 1 0\n",
      "[[0.41032539 0.58967461]] 0 0\n",
      "[[0.27557062 0.72442938]] 1 1\n",
      "[[0.50316341 0.49683659]] 0 1\n",
      "Recent Reward: 39.37\n",
      "Total Episodes: 500\n",
      "Total Steps: 14370\n",
      "\n",
      "-----------\n",
      "[[0.2115813 0.7884187]] 0 1\n",
      "[[0.63278487 0.36721513]] 0 0\n",
      "[[0.45826749 0.54173251]] 0 0\n",
      "[[0.72182357 0.27817643]] 1 0\n",
      "[[0.4655722 0.5344278]] 0 1\n",
      "[[0.38648935 0.61351065]] 0 0\n",
      "[[0.55525822 0.44474178]] 1 0\n",
      "[[0.21469055 0.78530945]] 1 1\n",
      "[[0.41348093 0.58651907]] 0 1\n",
      "Recent Reward: 44.5\n",
      "Total Episodes: 600\n",
      "Total Steps: 18820\n",
      "\n",
      "-----------\n",
      "[[0.50500881 0.49499119]] 1 1\n",
      "[[0.42412932 0.57587068]] 1 0\n",
      "[[0.58575346 0.41424654]] 0 1\n",
      "[[0.3093522 0.6906478]] 0 1\n",
      "[[0.47791589 0.52208411]] 1 0\n",
      "[[0.21679544 0.78320456]] 1 1\n",
      "[[0.27741687 0.72258313]] 0 1\n",
      "[[0.3958544 0.6041456]] 1 0\n",
      "[[0.50828108 0.49171892]] 1 1\n",
      "Recent Reward: 46.17\n",
      "Total Episodes: 700\n",
      "Total Steps: 23437\n",
      "\n",
      "-----------\n",
      "[[0.24589681 0.75410319]] 1 1\n",
      "[[0.46530118 0.53469882]] 1 1\n",
      "[[0.68458619 0.31541381]] 0 0\n",
      "[[0.63296262 0.36703738]] 1 1\n",
      "[[0.74789455 0.25210545]] 0 1\n",
      "[[0.71075544 0.28924456]] 0 0\n",
      "[[0.13126235 0.86873765]] 1 1\n",
      "[[0.92809087 0.07190913]] 0 0\n",
      "[[0.68724331 0.31275669]] 0 0\n",
      "[[0.84071121 0.15928879]] 1 1\n",
      "[[0.80345888 0.19654112]] 0 0\n",
      "Recent Reward: 51.89\n",
      "Total Episodes: 800\n",
      "Total Steps: 28626\n",
      "\n",
      "-----------\n",
      "[[0.41686595 0.58313405]] 1 0\n",
      "[[0.68334999 0.31665001]] 1 0\n",
      "[[0.7397489 0.2602511]] 0 1\n",
      "[[0.12916168 0.87083832]] 1 1\n",
      "[[0.59586741 0.40413259]] 0 0\n",
      "[[0.50318684 0.49681316]] 1 0\n",
      "[[0.36955557 0.63044443]] 1 1\n",
      "[[0.26398774 0.73601226]] 0 0\n",
      "[[0.79582078 0.20417922]] 0 0\n",
      "[[0.50427806 0.49572194]] 0 1\n",
      "[[0.42323787 0.57676213]] 0 0\n",
      "Recent Reward: 55.73\n",
      "Total Episodes: 900\n",
      "Total Steps: 34199\n",
      "\n",
      "-----------\n",
      "[[0.67927388 0.32072612]] 0 0\n",
      "[[0.60805127 0.39194873]] 1 1\n",
      "[[0.84019483 0.15980517]] 0 0\n",
      "[[0.22478397 0.77521603]] 1 0\n",
      "[[0.46733003 0.53266997]] 1 0\n",
      "[[0.47498559 0.52501441]] 1 1\n",
      "[[0.8448349 0.1551651]] 0 0\n",
      "[[0.61489519 0.38510481]] 0 1\n",
      "[[0.36536857 0.63463143]] 0 1\n",
      "[[0.42610498 0.57389502]] 0 0\n",
      "[[0.70539863 0.29460137]] 0 0\n",
      "[[0.81699265 0.18300735]] 0 0\n",
      "Recent Reward: 59.75\n",
      "Total Episodes: 1000\n",
      "Total Steps: 40174\n",
      "\n",
      "-----------\n",
      "[[0.70352065 0.29647935]] 0 0\n",
      "[[0.5196792 0.4803208]] 0 0\n",
      "[[0.4897668 0.5102332]] 0 0\n",
      "[[0.39001837 0.60998163]] 1 0\n",
      "[[0.51421433 0.48578567]] 1 0\n",
      "[[0.10534463 0.89465537]] 1 1\n",
      "[[0.94601584 0.05398416]] 0 0\n",
      "[[0.32441879 0.67558121]] 1 0\n",
      "[[0.26821429 0.73178571]] 1 1\n",
      "[[0.16838874 0.83161126]] 1 1\n",
      "[[0.61518537 0.38481463]] 1 0\n",
      "[[0.64378736 0.35621264]] 0 1\n",
      "[[0.24340611 0.75659389]] 1 1\n",
      "Recent Reward: 65.05\n",
      "Total Episodes: 1100\n",
      "Total Steps: 46679\n",
      "\n",
      "-----------\n",
      "[[0.63139009 0.36860991]] 0 1\n",
      "[[0.63793852 0.36206148]] 0 1\n",
      "[[0.57428946 0.42571054]] 0 1\n",
      "[[0.60403225 0.39596775]] 0 0\n",
      "[[0.52572435 0.47427565]] 0 0\n",
      "[[0.58594063 0.41405937]] 1 0\n",
      "[[0.48645734 0.51354266]] 1 0\n",
      "[[0.29647726 0.70352274]] 0 1\n",
      "[[0.54649708 0.45350292]] 1 0\n",
      "[[0.73190086 0.26809914]] 1 0\n",
      "[[0.49283854 0.50716146]] 0 0\n",
      "[[0.62780942 0.37219058]] 1 1\n",
      "[[0.2967819 0.7032181]] 1 0\n",
      "[[0.24178799 0.75821201]] 1 1\n",
      "[[0.65276414 0.34723586]] 1 0\n",
      "Recent Reward: 75.01\n",
      "Total Episodes: 1200\n",
      "Total Steps: 54180\n",
      "\n",
      "-----------\n",
      "[[0.87815725 0.12184275]] 0 1\n",
      "[[0.81101757 0.18898243]] 0 0\n",
      "[[0.16636974 0.83363026]] 1 0\n",
      "[[0.71387956 0.28612044]] 1 1\n",
      "[[0.56020079 0.43979921]] 1 0\n",
      "[[0.18745082 0.81254918]] 1 0\n",
      "[[0.34291864 0.65708136]] 1 0\n",
      "[[0.68765541 0.31234459]] 0 0\n",
      "[[0.66586507 0.33413493]] 0 1\n",
      "[[0.27707095 0.72292905]] 0 1\n",
      "[[0.63928369 0.36071631]] 0 0\n",
      "[[0.80596672 0.19403328]] 0 0\n",
      "[[0.60333654 0.39666346]] 0 1\n",
      "[[0.82939591 0.17060409]] 0 1\n",
      "Recent Reward: 70.21\n",
      "Total Episodes: 1300\n",
      "Total Steps: 61201\n",
      "\n",
      "-----------\n",
      "[[0.9151752 0.0848248]] 0 0\n",
      "[[0.30963648 0.69036352]] 1 1\n",
      "[[0.49228841 0.50771159]] 0 0\n",
      "[[0.65764783 0.34235217]] 1 0\n",
      "[[0.67269661 0.32730339]] 0 0\n",
      "[[0.49225541 0.50774459]] 1 0\n",
      "[[0.68963573 0.31036427]] 0 0\n",
      "[[0.76131728 0.23868272]] 1 0\n",
      "[[0.52297582 0.47702418]] 1 0\n",
      "[[0.65127943 0.34872057]] 0 0\n",
      "[[0.20671184 0.79328816]] 1 0\n",
      "[[0.33981876 0.66018124]] 1 1\n",
      "[[0.7001229 0.2998771]] 0 0\n",
      "[[0.81957949 0.18042051]] 0 1\n",
      "[[0.8637108 0.1362892]] 1 0\n",
      "[[0.39339589 0.60660411]] 1 0\n",
      "[[0.73944823 0.26055177]] 0 0\n",
      "Recent Reward: 85.68\n",
      "Total Episodes: 1400\n",
      "Total Steps: 69769\n",
      "\n",
      "-----------\n",
      "[[0.46532539 0.53467461]] 1 1\n",
      "[[0.43223293 0.56776707]] 1 0\n",
      "[[0.10764808 0.89235192]] 1 1\n",
      "[[0.13302388 0.86697612]] 1 1\n",
      "[[0.29308851 0.70691149]] 1 0\n",
      "[[0.25603953 0.74396047]] 1 1\n",
      "[[0.44309219 0.55690781]] 0 1\n",
      "[[0.76184609 0.23815391]] 0 0\n",
      "[[0.80065545 0.19934455]] 0 1\n",
      "[[0.77358345 0.22641655]] 1 1\n",
      "[[0.87423968 0.12576032]] 0 1\n",
      "[[0.28181809 0.71818191]] 1 1\n",
      "[[0.83064818 0.16935182]] 0 0\n",
      "[[0.90876939 0.09123061]] 0 0\n",
      "[[0.58678069 0.41321931]] 0 1\n",
      "[[0.78768589 0.21231411]] 1 1\n",
      "[[0.52372776 0.47627224]] 0 0\n",
      "[[0.22604255 0.77395745]] 1 0\n",
      "[[0.77015094 0.22984906]] 0 0\n",
      "Recent Reward: 94.31\n",
      "Total Episodes: 1500\n",
      "Total Steps: 79200\n",
      "\n",
      "-----------\n",
      "[[0.28932688 0.71067312]] 0 1\n",
      "[[0.32337971 0.67662029]] 1 1\n",
      "[[0.62819459 0.37180541]] 1 0\n",
      "[[0.55020582 0.44979418]] 0 1\n",
      "[[0.28705853 0.71294147]] 1 0\n",
      "[[0.30955975 0.69044025]] 1 0\n",
      "[[0.4345107 0.5654893]] 1 1\n",
      "[[0.78473249 0.21526751]] 0 0\n",
      "[[0.8517772 0.1482228]] 1 1\n",
      "[[0.56272942 0.43727058]] 0 1\n",
      "[[0.45962139 0.54037861]] 1 0\n",
      "[[0.43854702 0.56145298]] 1 0\n",
      "[[0.69981799 0.30018201]] 0 1\n",
      "[[0.50356895 0.49643105]] 0 0\n",
      "[[0.23265412 0.76734588]] 1 1\n",
      "[[0.63328558 0.36671442]] 1 0\n",
      "[[0.51331704 0.48668296]] 0 0\n",
      "[[0.69626444 0.30373556]] 0 0\n",
      "[[0.55348404 0.44651596]] 1 0\n",
      "[[0.12671261 0.87328739]] 1 1\n",
      "[[0.2613141 0.7386859]] 1 1\n",
      "[[0.88082583 0.11917417]] 0 1\n",
      "Recent Reward: 112.96\n",
      "Total Episodes: 1600\n",
      "Total Steps: 90496\n",
      "\n",
      "-----------\n",
      "[[0.39908662 0.60091338]] 0 0\n",
      "[[0.51826701 0.48173299]] 1 1\n",
      "[[0.64322874 0.35677126]] 0 0\n",
      "[[0.80960251 0.19039749]] 0 0\n",
      "[[0.7077788 0.2922212]] 0 0\n",
      "[[0.37699461 0.62300539]] 1 0\n",
      "[[0.30149562 0.69850438]] 0 0\n",
      "[[0.57136798 0.42863202]] 0 1\n",
      "[[0.15242132 0.84757868]] 1 1\n",
      "[[0.4920996 0.5079004]] 1 1\n",
      "[[0.71698027 0.28301973]] 0 0\n",
      "[[0.06754854 0.93245146]] 1 1\n",
      "[[0.14915942 0.85084058]] 1 1\n",
      "[[0.09974409 0.90025591]] 1 0\n",
      "[[0.78393668 0.21606332]] 0 0\n",
      "[[0.24785789 0.75214211]] 0 0\n",
      "[[0.56587019 0.43412981]] 0 1\n",
      "[[0.72252563 0.27747437]] 0 0\n",
      "[[0.24963645 0.75036355]] 1 1\n",
      "[[0.51826667 0.48173333]] 0 0\n",
      "[[0.23853201 0.76146799]] 1 1\n",
      "[[0.84242823 0.15757177]] 0 0\n",
      "[[0.79612411 0.20387589]] 0 1\n",
      "[[0.51725738 0.48274262]] 1 1\n",
      "[[0.45929701 0.54070299]] 1 1\n",
      "Recent Reward: 123.36\n",
      "Total Episodes: 1700\n",
      "Total Steps: 102832\n",
      "\n",
      "-----------\n",
      "[[0.48754572 0.51245428]] 1 1\n",
      "[[0.93089667 0.06910333]] 0 1\n",
      "[[0.5269232 0.4730768]] 0 0\n",
      "[[0.17695331 0.82304669]] 1 0\n",
      "[[0.6200941 0.3799059]] 1 0\n",
      "[[0.15630207 0.84369793]] 1 0\n",
      "[[0.45569631 0.54430369]] 0 1\n",
      "[[0.34473578 0.65526422]] 0 1\n",
      "[[0.05828914 0.94171086]] 1 0\n",
      "[[0.56048087 0.43951913]] 1 0\n",
      "[[0.16724044 0.83275956]] 1 0\n",
      "[[0.71940611 0.28059389]] 1 1\n",
      "[[0.31417955 0.68582045]] 0 1\n",
      "[[0.13372335 0.86627665]] 1 1\n",
      "[[0.38432057 0.61567943]] 1 1\n",
      "[[0.15300655 0.84699345]] 1 0\n",
      "[[0.24717767 0.75282233]] 1 0\n",
      "[[0.5499361 0.4500639]] 0 0\n",
      "[[0.58601426 0.41398574]] 1 0\n",
      "[[0.660515 0.339485]] 1 1\n",
      "[[0.33486498 0.66513502]] 1 0\n",
      "[[0.23883619 0.76116381]] 1 1\n",
      "[[0.07502642 0.92497358]] 1 1\n",
      "[[0.07506106 0.92493894]] 1 1\n",
      "[[0.38549851 0.61450149]] 1 1\n",
      "Recent Reward: 125.08\n",
      "Total Episodes: 1800\n",
      "Total Steps: 115340\n",
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69062745 0.30937255]] 0 0\n",
      "[[0.82523708 0.17476292]] 0 0\n",
      "[[0.50934616 0.49065384]] 1 0\n",
      "[[0.64818268 0.35181732]] 0 1\n",
      "[[0.629458 0.370542]] 1 1\n",
      "[[0.3469152 0.6530848]] 1 0\n",
      "[[0.96352979 0.03647021]] 0 0\n",
      "[[0.39988549 0.60011451]] 1 1\n",
      "[[0.7137062 0.2862938]] 0 0\n",
      "[[0.39190754 0.60809246]] 1 1\n",
      "[[0.43707486 0.56292514]] 1 0\n",
      "[[0.20588646 0.79411354]] 0 0\n",
      "[[0.27759081 0.72240919]] 1 0\n",
      "[[0.74197727 0.25802273]] 0 1\n",
      "[[0.11352292 0.88647708]] 1 1\n",
      "[[0.73603044 0.26396956]] 0 1\n",
      "[[0.73703733 0.26296267]] 0 1\n",
      "[[0.77455246 0.22544754]] 0 0\n",
      "[[0.39698152 0.60301848]] 1 0\n",
      "[[0.67512671 0.32487329]] 0 1\n",
      "[[0.47255253 0.52744747]] 1 0\n",
      "[[0.09307498 0.90692502]] 1 1\n",
      "[[0.68296504 0.31703496]] 1 1\n",
      "[[0.79970019 0.20029981]] 0 0\n",
      "[[0.14418392 0.85581608]] 1 0\n",
      "[[0.35664519 0.64335481]] 1 1\n",
      "[[0.26607897 0.73392103]] 1 1\n",
      "[[0.25022528 0.74977472]] 0 0\n",
      "[[0.14957508 0.85042492]] 1 0\n",
      "Recent Reward: 146.37\n",
      "Total Episodes: 1900\n",
      "Total Steps: 129977\n",
      "\n",
      "-----------\n",
      "[[0.57925102 0.42074898]] 0 0\n",
      "[[0.89818542 0.10181458]] 0 0\n",
      "[[0.49213413 0.50786587]] 0 0\n",
      "[[0.15561276 0.84438724]] 1 1\n",
      "[[0.72796683 0.27203317]] 1 0\n",
      "[[0.7320232 0.2679768]] 0 1\n",
      "[[0.27777614 0.72222386]] 0 1\n",
      "[[0.35778792 0.64221208]] 1 1\n",
      "[[0.7631502 0.2368498]] 0 0\n",
      "[[0.23968976 0.76031024]] 0 1\n",
      "[[0.26916865 0.73083135]] 1 1\n",
      "[[0.69285884 0.30714116]] 0 1\n",
      "[[0.49281643 0.50718357]] 0 0\n",
      "[[0.57672393 0.42327607]] 0 0\n",
      "[[0.2139865 0.7860135]] 1 0\n",
      "[[0.79417937 0.20582063]] 1 0\n",
      "[[0.87219879 0.12780121]] 0 1\n",
      "[[0.39472088 0.60527912]] 1 1\n",
      "[[0.51595388 0.48404612]] 0 0\n",
      "[[0.6789824 0.3210176]] 0 1\n",
      "[[0.80978731 0.19021269]] 1 1\n",
      "[[0.87266726 0.12733274]] 0 1\n",
      "[[0.32684451 0.67315549]] 1 1\n",
      "[[0.28114297 0.71885703]] 0 0\n",
      "[[0.45500576 0.54499424]] 1 1\n",
      "[[0.89269786 0.10730214]] 0 0\n",
      "[[0.36554574 0.63445426]] 0 1\n",
      "[[0.93434101 0.06565899]] 0 1\n",
      "[[0.30683343 0.69316657]] 1 1\n",
      "Recent Reward: 143.61\n",
      "Total Episodes: 2000\n",
      "Total Steps: 144338\n",
      "\n",
      "-----------\n",
      "[[0.54125954 0.45874046]] 0 1\n",
      "[[0.59497197 0.40502803]] 0 1\n",
      "[[0.33587759 0.66412241]] 0 1\n",
      "[[0.1934037 0.8065963]] 1 0\n",
      "[[0.35340212 0.64659788]] 0 1\n",
      "[[0.10695204 0.89304796]] 1 1\n",
      "[[0.22956748 0.77043252]] 1 1\n",
      "[[0.63581635 0.36418365]] 0 1\n",
      "[[0.19562056 0.80437944]] 1 1\n",
      "[[0.39135868 0.60864132]] 1 0\n",
      "[[0.152313 0.847687]] 1 1\n",
      "[[0.80217151 0.19782849]] 0 0\n",
      "[[0.86363966 0.13636034]] 0 0\n",
      "[[0.68125613 0.31874387]] 0 0\n",
      "[[0.42324217 0.57675783]] 0 1\n",
      "[[0.64645264 0.35354736]] 1 1\n",
      "[[0.60821691 0.39178309]] 0 1\n",
      "[[0.35563567 0.64436433]] 1 1\n",
      "[[0.88447933 0.11552067]] 0 1\n",
      "[[0.75467727 0.24532273]] 0 0\n",
      "[[0.58262531 0.41737469]] 1 0\n",
      "[[0.83968598 0.16031402]] 0 1\n",
      "[[0.37653436 0.62346564]] 0 1\n",
      "[[0.87796296 0.12203704]] 0 0\n",
      "[[0.46830615 0.53169385]] 1 0\n",
      "[[0.88715165 0.11284835]] 0 1\n",
      "[[0.53100197 0.46899803]] 0 0\n",
      "[[0.29214203 0.70785797]] 0 1\n",
      "[[0.59531332 0.40468668]] 1 1\n",
      "[[0.48478034 0.51521966]] 1 1\n",
      "[[0.69776358 0.30223642]] 0 0\n",
      "Recent Reward: 152.96\n",
      "Total Episodes: 2100\n",
      "Total Steps: 159634\n",
      "\n",
      "-----------\n",
      "[[0.6439783 0.3560217]] 1 0\n",
      "[[0.31701709 0.68298291]] 0 0\n",
      "[[0.56253199 0.43746801]] 1 0\n",
      "[[0.7156782 0.2843218]] 1 0\n",
      "[[0.44968393 0.55031607]] 1 0\n",
      "[[0.89825768 0.10174232]] 0 0\n",
      "[[0.66308481 0.33691519]] 0 0\n",
      "[[0.53450147 0.46549853]] 0 1\n",
      "[[0.34449153 0.65550847]] 0 1\n",
      "[[0.73987642 0.26012358]] 0 0\n",
      "[[0.66767934 0.33232066]] 1 1\n",
      "[[0.31624628 0.68375372]] 0 1\n",
      "[[0.85191324 0.14808676]] 1 1\n",
      "[[0.77624812 0.22375188]] 1 0\n",
      "[[0.52868476 0.47131524]] 1 0\n",
      "[[0.90936019 0.09063981]] 0 0\n",
      "[[0.63146032 0.36853968]] 1 1\n",
      "[[0.32374252 0.67625748]] 1 1\n",
      "[[0.56792888 0.43207112]] 1 0\n",
      "[[0.59015602 0.40984398]] 0 0\n",
      "[[0.05601165 0.94398835]] 1 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-209b0e129737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mact2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a683d28ee5ef>\u001b[0m in \u001b[0;36mchoose_action2\u001b[0;34m(self, obs, sess)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "total_steps = 0\n",
    "total_episodes = 0\n",
    "all_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    mb.start_rollout()\n",
    "    for step in range(max_steps):\n",
    "        act, _ = network.choose_action2([obs])\n",
    "        act2 = network.choose_action1([obs])[0]\n",
    "        \n",
    "        if total_steps % 500 == 0:\n",
    "            print(_, act, act2)\n",
    "\n",
    "        # print(act)\n",
    "        # env.render()\n",
    "        # time.sleep(0.01)\n",
    "        obs_next, rew, d, _ = env.step(act)\n",
    "        episode_reward += rew\n",
    "        \n",
    "        mb.record(obs, act, rew)\n",
    "        obs = obs_next\n",
    "        \n",
    "        total_steps += 1\n",
    "        if d: # or total_steps % update_freq == 0:\n",
    "            #  print(act)\n",
    "            break\n",
    "    # print(episode_reward)\n",
    "    all_rewards.append(episode_reward)\n",
    "    total_episodes += 1\n",
    "            \n",
    "    if total_episodes % 100 == 0:\n",
    "        print('Recent Reward:', np.mean(all_rewards[-100:]))\n",
    "        print('Total Episodes:', total_episodes)\n",
    "        print('Total Steps:', total_steps)\n",
    "        print('\\n-----------')\n",
    "        \n",
    "        train_data = mb.to_data()\n",
    "#         rews = train_data[:,2]\n",
    "#         train_data[:,2] = (rews - np.mean(rews)) / np.std(rews)\n",
    "        network.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.end_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.to_data().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
