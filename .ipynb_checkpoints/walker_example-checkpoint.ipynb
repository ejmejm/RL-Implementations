{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import MemoryBuffer\n",
    "from policies import PPOTrainer\n",
    "from utils import gaussian_likelihood, reshape_train_var\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense, conv2d, max_pooling2d, flatten\n",
    "import numpy as np\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMTrainer():\n",
    "    def __init__(self, in_op, out_op, v_out_op, act_type='discrete', sess=None):\n",
    "        \"\"\"\n",
    "        Create a wrapper for RL networks for easy training.\n",
    "        Args:\n",
    "            in_op (tf.Placeholder): Observation input to architecture\n",
    "            out_op (tf.Variable): Action output of architecture\n",
    "            act_type (string): 'discrete' for a discrete actions space or 'continuous'\n",
    "                               for a continuous actions space\n",
    "            sess (tf.Session): A session if you would like to use a custom session,\n",
    "                               if left none it will be automatically created\n",
    "        \"\"\"\n",
    "\n",
    "        if not sess:\n",
    "            self.renew_sess()\n",
    "        \n",
    "        self.in_op = in_op\n",
    "        self.out_op = out_op\n",
    "        self.v_out_op = v_out_op\n",
    "        \n",
    "        if act_type in ('discrete', 'd'):\n",
    "            self.train = self._create_discrete_trainer()\n",
    "            self.act_type = 'discrete'\n",
    "        elif act_type in ('continuous', 'c'):\n",
    "            self.train = self._create_continuous_trainer()\n",
    "            self.act_type = 'continuous'\n",
    "        else:\n",
    "            raise TypeError('act_type must be \\'discrete\\' or \\'continuous\\'')\n",
    "        \n",
    "    def renew_sess(self):\n",
    "        \"\"\"\n",
    "        Starts a new internal Tensorflow session\n",
    "        \"\"\"\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def end_sess(self):\n",
    "        \"\"\"\n",
    "        Ends the internal Tensorflow session if it exists\n",
    "        \"\"\"\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "            \n",
    "    def _create_ICM(self):\n",
    "        feature_dim = 12\n",
    "        r_i_scale = 3\n",
    "        \n",
    "        # Create placeholder\n",
    "        self.next_obs_holders = tf.placeholder(tf.float32, shape=self.in_op.shape)\n",
    "        \n",
    "        # Observation feature encoder\n",
    "        with tf.variable_scope('feature_encoder'):\n",
    "            self.f_obs = dense(self.in_op, feature_dim, activation=tf.nn.tanh, name='fe_dense')\n",
    "            \n",
    "        with tf.variable_scope('feature_encoder', reuse=True):\n",
    "            self.f_obs_next = dense(self.next_obs_holders, feature_dim, activation=tf.nn.tanh, name='fe_dense')\n",
    "            \n",
    "        # State predictor forward model\n",
    "        self.state_act_pair = tf.concat([self.act_holders, self.f_obs], axis=1)\n",
    "        self.sp_dense = dense(self.state_act_pair, 32, activation=tf.nn.tanh)\n",
    "        self.f_obs_next_hat = dense(self.sp_dense, feature_dim, activation=tf.nn.tanh)\n",
    "        \n",
    "        # Inverse model (predicting action)\n",
    "        self.state_state_pair = tf.concat([self.f_obs, self.f_obs_next], axis=1)\n",
    "        self.act_hat = dense(self.state_state_pair, self.out_op.shape[1])\n",
    "        \n",
    "        # Calculating intrinsic reward\n",
    "        self.obs_diff = self.f_obs_next_hat - self.f_obs_next\n",
    "        self.r_i = r_i_scale * tf.reduce_sum(self.obs_diff ** 2, axis=1)\n",
    "        \n",
    "        # Calculating losses\n",
    "        self.pre_loss_i = tf.reduce_sum((self.act_hat - self.act_holders) ** 2, axis=1)\n",
    "        self.pre_loss_f = tf.reduce_sum(self.obs_diff ** 2, axis=1)\n",
    "        \n",
    "        self.loss_i = 0.5 * self.pre_loss_i\n",
    "        self.loss_f = 0.5 * self.pre_loss_f\n",
    "        \n",
    "    def _create_discrete_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a discrete action space\n",
    "        \"\"\"\n",
    "#         self.act_holders = tf.placeholder(tf.int32, shape=[None])\n",
    "#         self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "#         self.act_masks = tf.one_hot(self.act_holders, self.out_op.shape[1].value, dtype=tf.float32)\n",
    "#         self.log_probs = tf.log(self.out_op)\n",
    "        \n",
    "#         self.advantages = self.reward_holders - tf.squeeze(self.v_out_op)\n",
    "        \n",
    "#         self.resp_acts = tf.reduce_sum(self.act_masks *  self.log_probs, axis=1)\n",
    "#         self.loss = -tf.reduce_mean(self.resp_acts * self.advantages)\n",
    "        \n",
    "#         self.optimizer = optimizer\n",
    "#         self.actor_update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "#         with tf.control_dependencies([self.actor_update]):\n",
    "#             self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.v_out_op)))\n",
    "#             self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "#         update_func = lambda train_data: self.sess.run([self.actor_update, self.value_update], \n",
    "#                                                        feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "#                                                             self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "#                                                             self.reward_holders: train_data[:, 2]})\n",
    "        \n",
    "#         self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "#         return update_func\n",
    "        \n",
    "    def _create_continuous_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a continuous action space\n",
    "        \"\"\"\n",
    "        self.act_holders = tf.placeholder(tf.float32, shape=[None, self.out_op.shape[1].value])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.std = tf.Variable(0.5 * np.ones(shape=self.out_op.shape[1].value), dtype=tf.float32)\n",
    "        self.out_act = self.out_op + tf.random_normal(tf.shape(self.out_op), dtype=tf.float32) * self.std\n",
    "        \n",
    "        self.log_probs = gaussian_likelihood(self.act_holders, self.out_op, self.std)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.v_out_op)\n",
    "        \n",
    "        # Creation of ICM module\n",
    "        \n",
    "        self._create_ICM()\n",
    "        \n",
    "        self.r_e = tf.reduce_mean(self.log_probs * self.advantages)\n",
    "        self.r_t = tf.reduce_mean(self.r_e + self.r_i) # Maybe I should reduce_sum or split by rollout\n",
    "        \n",
    "        self.lamb = 0.1\n",
    "        self.beta = 0.2\n",
    "        \n",
    "        self.total_loss = -self.lamb * self.r_t + (1.-self.beta) * self.loss_i + self.beta * self.loss_f\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.actor_update = self.optimizer.minimize(self.total_loss)\n",
    "        \n",
    "        with tf.control_dependencies([self.actor_update]):\n",
    "            self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.v_out_op)))\n",
    "            self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        def update_func(train_data):\n",
    "            i_reward, _, _ = self.sess.run([self.r_i, self.actor_update, self.value_update], \n",
    "                               feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                    self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                    self.reward_holders: train_data[:, 2],\n",
    "                                    self.next_obs_holders: reshape_train_var(train_data[:, 3])})\n",
    "            \n",
    "            print(f'Intrinsic Reward: {np.mean(i_reward)}')    \n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _gen_discrete_act(self, obs):\n",
    "        act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
    "        act = np.random.choice(list(range(len(act_probs[0]))), p=act_probs)\n",
    "        \n",
    "        return act\n",
    "    \n",
    "    def _gen_continuous_act(self, obs):\n",
    "        act_vect = self.sess.run(self.out_act, feed_dict={self.in_op: [obs]})[0]\n",
    "        \n",
    "        return np.array(act_vect)\n",
    "        \n",
    "    def gen_act(self, obs):\n",
    "        if self.act_type == 'discrete':\n",
    "            return self._gen_discrete_act(obs)\n",
    "        else:\n",
    "            return self._gen_continuous_act(obs)\n",
    "        \n",
    "    def train(self, obs, rewards, acts):\n",
    "        raise RuntimeError('The train method was not properly created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "obs = tf.placeholder(tf.float32, shape=[None]+list(env.observation_space.shape))\n",
    "dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "dense2 = dense(dense1, 32, activation=tf.tanh)\n",
    "act_probs = dense(dense2, env.action_space.shape[0])\n",
    "\n",
    "v_dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "v_dense2 = dense(v_dense1, 32, activation=tf.tanh)\n",
    "value = dense(v_dense2, 1)\n",
    "\n",
    "network = IMTrainer(obs, act_probs, value, act_type='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000000\n",
    "max_steps = 500\n",
    "update_freq = 1\n",
    "print_freq = 1\n",
    "\n",
    "mb = MemoryBuffer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Reward: 141.8203582763672\n",
      "Update #1, Reward: -108.1877613371412\n",
      "\n",
      "Intrinsic Reward: 142.29800415039062\n",
      "Update #2, Reward: -104.93920621598326\n",
      "\n",
      "Intrinsic Reward: 141.77227783203125\n",
      "Update #3, Reward: -106.67962389740099\n",
      "\n",
      "Intrinsic Reward: 141.9000701904297\n",
      "Update #4, Reward: -104.33232116363818\n",
      "\n",
      "Intrinsic Reward: 140.94427490234375\n",
      "Update #5, Reward: -107.94131572361849\n",
      "\n",
      "Intrinsic Reward: 142.08303833007812\n",
      "Update #6, Reward: -105.70714224652387\n",
      "\n",
      "Intrinsic Reward: 142.25411987304688\n",
      "Update #7, Reward: -103.10124309706129\n",
      "\n",
      "Intrinsic Reward: 142.13668823242188\n",
      "Update #8, Reward: -105.26173562503979\n",
      "\n",
      "Intrinsic Reward: 141.6094970703125\n",
      "Update #9, Reward: -103.0819227867201\n",
      "\n",
      "Intrinsic Reward: 141.536376953125\n",
      "Update #10, Reward: -105.80744348400646\n",
      "\n",
      "Intrinsic Reward: 141.97628784179688\n",
      "Update #11, Reward: -106.28764034203999\n",
      "\n",
      "Intrinsic Reward: 141.01806640625\n",
      "Update #12, Reward: -104.41805487677144\n",
      "\n",
      "Intrinsic Reward: 141.51329040527344\n",
      "Update #13, Reward: -104.37899782687549\n",
      "\n",
      "Intrinsic Reward: 141.35781860351562\n",
      "Update #14, Reward: -106.57138668208196\n",
      "\n",
      "Intrinsic Reward: 142.32846069335938\n",
      "Update #15, Reward: -105.59556255232295\n",
      "\n",
      "Intrinsic Reward: 141.1341552734375\n",
      "Update #16, Reward: -108.6164878128767\n",
      "\n",
      "Intrinsic Reward: 142.519287109375\n",
      "Update #17, Reward: -104.74803597872952\n",
      "\n",
      "Intrinsic Reward: 140.8370819091797\n",
      "Update #18, Reward: -104.28417317432414\n",
      "\n",
      "Intrinsic Reward: 141.9203643798828\n",
      "Update #19, Reward: -115.61239486360923\n",
      "\n",
      "Intrinsic Reward: 141.69227600097656\n",
      "Update #20, Reward: -103.88285392614144\n",
      "\n",
      "Intrinsic Reward: 141.79876708984375\n",
      "Update #21, Reward: -106.76156608944945\n",
      "\n",
      "Intrinsic Reward: 141.47683715820312\n",
      "Update #22, Reward: -105.90478473294526\n",
      "\n",
      "Intrinsic Reward: 141.8109130859375\n",
      "Update #23, Reward: -107.22677551617598\n",
      "\n",
      "Intrinsic Reward: 141.5924072265625\n",
      "Update #24, Reward: -106.76445467219627\n",
      "\n",
      "Intrinsic Reward: 141.38446044921875\n",
      "Update #25, Reward: -108.15315038129935\n",
      "\n",
      "Intrinsic Reward: 141.57150268554688\n",
      "Update #26, Reward: -102.64823770643399\n",
      "\n",
      "Intrinsic Reward: 141.63499450683594\n",
      "Update #27, Reward: -105.13130703084543\n",
      "\n",
      "Intrinsic Reward: 141.43438720703125\n",
      "Update #28, Reward: -104.74440704503034\n",
      "\n",
      "Intrinsic Reward: 141.8336944580078\n",
      "Update #29, Reward: -108.1413591074242\n",
      "\n",
      "Intrinsic Reward: 141.57736206054688\n",
      "Update #30, Reward: -106.11427061746518\n",
      "\n",
      "Intrinsic Reward: 142.0178985595703\n",
      "Update #31, Reward: -105.25675423040987\n",
      "\n",
      "Intrinsic Reward: 141.38351440429688\n",
      "Update #32, Reward: -111.054811635095\n",
      "\n",
      "Intrinsic Reward: 141.6923065185547\n",
      "Update #33, Reward: -106.97965626954412\n",
      "\n",
      "Intrinsic Reward: 143.0264129638672\n",
      "Update #34, Reward: -36.9899270577468\n",
      "\n",
      "Intrinsic Reward: 141.08181762695312\n",
      "Update #35, Reward: -104.48424061308614\n",
      "\n",
      "Intrinsic Reward: 141.84469604492188\n",
      "Update #36, Reward: -109.00574098489744\n",
      "\n",
      "Intrinsic Reward: 141.97576904296875\n",
      "Update #37, Reward: -109.09896906596919\n",
      "\n",
      "Intrinsic Reward: 142.1649169921875\n",
      "Update #38, Reward: -106.34159368145342\n",
      "\n",
      "Intrinsic Reward: 142.7912139892578\n",
      "Update #39, Reward: -106.39288703279446\n",
      "\n",
      "Intrinsic Reward: 141.34768676757812\n",
      "Update #40, Reward: -103.6071144979832\n",
      "\n",
      "Intrinsic Reward: 141.0658721923828\n",
      "Update #41, Reward: -104.6194144756974\n",
      "\n",
      "Intrinsic Reward: 141.61529541015625\n",
      "Update #42, Reward: -105.420494292972\n",
      "\n",
      "Intrinsic Reward: 141.40487670898438\n",
      "Update #43, Reward: -106.30568347445316\n",
      "\n",
      "Intrinsic Reward: 140.71743774414062\n",
      "Update #44, Reward: -105.47180014954694\n",
      "\n",
      "Intrinsic Reward: 140.70042419433594\n",
      "Update #45, Reward: -108.20182048358582\n",
      "\n",
      "Intrinsic Reward: 141.75291442871094\n",
      "Update #46, Reward: -104.96513709293306\n",
      "\n",
      "Intrinsic Reward: 141.4163818359375\n",
      "Update #47, Reward: -103.2083711185716\n",
      "\n",
      "Intrinsic Reward: 141.5734405517578\n",
      "Update #48, Reward: -105.7859948893624\n",
      "\n",
      "Intrinsic Reward: 141.19920349121094\n",
      "Update #49, Reward: -105.26336716277153\n",
      "\n",
      "Intrinsic Reward: 141.47544860839844\n",
      "Update #50, Reward: -104.20614687557642\n",
      "\n",
      "Intrinsic Reward: 142.30235290527344\n",
      "Update #51, Reward: -105.20630229712215\n",
      "\n",
      "Intrinsic Reward: 141.98336791992188\n",
      "Update #52, Reward: -107.9824453224397\n",
      "\n",
      "Intrinsic Reward: 142.40843200683594\n",
      "Update #53, Reward: -104.72785952285305\n",
      "\n",
      "Intrinsic Reward: 141.37106323242188\n",
      "Update #54, Reward: -106.0195502464405\n",
      "\n",
      "Intrinsic Reward: 142.6260528564453\n",
      "Update #55, Reward: -105.71227191124422\n",
      "\n",
      "Intrinsic Reward: 141.02088928222656\n",
      "Update #56, Reward: -103.06099650029962\n",
      "\n",
      "Intrinsic Reward: 142.07620239257812\n",
      "Update #57, Reward: -101.82949733481618\n",
      "\n",
      "Intrinsic Reward: 141.6961669921875\n",
      "Update #58, Reward: -106.16414605190792\n",
      "\n",
      "Intrinsic Reward: 141.37814331054688\n",
      "Update #59, Reward: -108.09621202900634\n",
      "\n",
      "Intrinsic Reward: 141.221923828125\n",
      "Update #60, Reward: -104.77135073751894\n",
      "\n",
      "Intrinsic Reward: 140.2185516357422\n",
      "Update #61, Reward: -105.80002406729758\n",
      "\n",
      "Intrinsic Reward: 141.69847106933594\n",
      "Update #62, Reward: -107.45094949322257\n",
      "\n",
      "Intrinsic Reward: 141.75473022460938\n",
      "Update #63, Reward: -106.39284998597391\n",
      "\n",
      "Intrinsic Reward: 141.5225372314453\n",
      "Update #64, Reward: -102.1082591671801\n",
      "\n",
      "Intrinsic Reward: 141.15158081054688\n",
      "Update #65, Reward: -108.07727464465424\n",
      "\n",
      "Intrinsic Reward: 140.45919799804688\n",
      "Update #66, Reward: -105.57491119013665\n",
      "\n",
      "Intrinsic Reward: 141.44723510742188\n",
      "Update #67, Reward: -103.1706391663508\n",
      "\n",
      "Intrinsic Reward: 140.24981689453125\n",
      "Update #68, Reward: -109.99899242916827\n",
      "\n",
      "Intrinsic Reward: 141.08673095703125\n",
      "Update #69, Reward: -102.67401169965478\n",
      "\n",
      "Intrinsic Reward: 141.17507934570312\n",
      "Update #70, Reward: -104.95828956751711\n",
      "\n",
      "Intrinsic Reward: 141.4047088623047\n",
      "Update #71, Reward: -108.33259769498247\n",
      "\n",
      "Intrinsic Reward: 140.59458923339844\n",
      "Update #72, Reward: -106.90231296114312\n",
      "\n",
      "Intrinsic Reward: 142.11422729492188\n",
      "Update #73, Reward: -106.76366819421202\n",
      "\n",
      "Intrinsic Reward: 141.4229736328125\n",
      "Update #74, Reward: -107.32587377505439\n",
      "\n",
      "Intrinsic Reward: 141.5803985595703\n",
      "Update #75, Reward: -105.47701288139882\n",
      "\n",
      "Intrinsic Reward: 141.6165771484375\n",
      "Update #76, Reward: -105.1580225481571\n",
      "\n",
      "Intrinsic Reward: 142.01980590820312\n",
      "Update #77, Reward: -104.5362838189161\n",
      "\n",
      "Intrinsic Reward: 141.6065673828125\n",
      "Update #78, Reward: -103.27893150696468\n",
      "\n",
      "Intrinsic Reward: 141.5815887451172\n",
      "Update #79, Reward: -110.46927997660389\n",
      "\n",
      "Intrinsic Reward: 141.5307159423828\n",
      "Update #80, Reward: -105.49976420321067\n",
      "\n",
      "Intrinsic Reward: 141.35955810546875\n",
      "Update #81, Reward: -106.8355542401423\n",
      "\n",
      "Intrinsic Reward: 141.16329956054688\n",
      "Update #82, Reward: -105.45847873457397\n",
      "\n",
      "Intrinsic Reward: 140.41529846191406\n",
      "Update #83, Reward: -105.49389006437609\n",
      "\n",
      "Intrinsic Reward: 141.62979125976562\n",
      "Update #84, Reward: -107.15140830093746\n",
      "\n",
      "Intrinsic Reward: 141.5764923095703\n",
      "Update #85, Reward: -110.1154617783775\n",
      "\n",
      "Intrinsic Reward: 140.95977783203125\n",
      "Update #86, Reward: -105.30117700071011\n",
      "\n",
      "Intrinsic Reward: 142.27137756347656\n",
      "Update #87, Reward: -105.45674525890934\n",
      "\n",
      "Intrinsic Reward: 141.89593505859375\n",
      "Update #88, Reward: -106.67727113558601\n",
      "\n",
      "Intrinsic Reward: 142.5494842529297\n",
      "Update #89, Reward: -104.73316090403374\n",
      "\n",
      "Intrinsic Reward: 141.5731964111328\n",
      "Update #90, Reward: -104.76763841399799\n",
      "\n",
      "Intrinsic Reward: 142.24366760253906\n",
      "Update #91, Reward: -108.15106970007594\n",
      "\n",
      "Intrinsic Reward: 142.0114288330078\n",
      "Update #92, Reward: -107.25361602476177\n",
      "\n",
      "Intrinsic Reward: 141.61012268066406\n",
      "Update #93, Reward: -104.43919029539761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    ep_reward = 0\n",
    "    \n",
    "    mb.start_rollout()\n",
    "    obs = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        obs = obs.squeeze()\n",
    "        act = network.gen_act(obs)\n",
    "        \n",
    "        obs_next, rew, d, _ = env.step(act)\n",
    "        ep_reward += rew\n",
    "        \n",
    "        if False:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        mb.record(obs, act, rew, obs_next)\n",
    "        obs = obs_next\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "            \n",
    "    all_rewards.append(ep_reward)\n",
    "            \n",
    "    if episode % update_freq == 0 and episode != 0:\n",
    "        network.train(mb.to_data())\n",
    "        \n",
    "        if episode % (update_freq * print_freq) == 0:\n",
    "            print(f'Update #{episode // update_freq}, Reward: {np.mean(all_rewards[-update_freq*print_freq:])}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
