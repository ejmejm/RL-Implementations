{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import MTMemoryBuffer\n",
    "from policies import PPOTrainer\n",
    "from env_control import EnvController\n",
    "from utils import gaussian_likelihood, reshape_train_var\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense, conv2d, max_pooling2d, flatten\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from env import CartPoleEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv()\n",
    "\n",
    "obs = tf.placeholder(tf.float32, shape=[None]+list(env.observation_space.shape))\n",
    "dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "dense2 = dense(dense1, 32, activation=tf.tanh)\n",
    "act_probs = dense(dense2, env.action_space.shape[0])\n",
    "\n",
    "v_dense1 = dense(obs, 32, activation=tf.tanh)\n",
    "v_dense2 = dense(v_dense1, 32, activation=tf.tanh)\n",
    "value = dense(v_dense2, 1)\n",
    "\n",
    "network = PPOTrainer(obs, act_probs, value, act_type='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = EnvController(CartPoleEnv, n_threads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.090187072753906"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = time.time()\n",
    "ec.sim_thread(1, network, 1000)\n",
    "time.time() - tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.007272005081177"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = time.time()\n",
    "ec.sim_episodes(network, 1000)\n",
    "time.time() - tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EnvController' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-de585afe81ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCartPoleEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EnvController' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    ec = EnvController(CartPoleEnv, mb, n_threads=i)\n",
    "    tmp = time.time()\n",
    "    ec.sim_episodes(network, 1000)\n",
    "    print(i, ':', time.time() - tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_agent(agent_id, create_env, network, mb, n_episodes=4, max_steps=500, render=False):\n",
    "    env = create_env()\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        mb.start_rollout(agent_id)\n",
    "        for step in range(max_steps):\n",
    "            act = network.gen_act([obs])\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.02)\n",
    "                \n",
    "            obs_next, rew, d, _ = env.step(act)\n",
    "            \n",
    "            mb.record(agent_id, obs, act, rew)\n",
    "            obs = obs_next\n",
    "            \n",
    "            if d:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(mpmb, n_threads=7, n_episodes=4, return_rewards=False, reset_mem=True):\n",
    "    agent_pool = []\n",
    "    \n",
    "    for i in range(n_threads):\n",
    "        agent_pool.append(Thread(target=spawn_agent, args=(i, make_lunar_lander_c, network.gen_act, mpmb),\n",
    "                                                     kwargs={'n_episodes': math.ceil(n_episodes/n_threads)}))\n",
    "        agent_pool[-1].start()\n",
    "\n",
    "    for agent in agent_pool:\n",
    "        agent.join()\n",
    "        \n",
    "    \n",
    "    if return_rewards:\n",
    "        return mpmb.get_avg_reward(), mpmb.to_data(reset=reset_mem)\n",
    "    return mpmb.to_data(reset=reset_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpmb = MTMemoryBuffer()\n",
    "# ep_rewards = []\n",
    "\n",
    "# for i in range(n_episodes//update_freq):\n",
    "# #     ep_reward, train_data = gather_data(mpmb, n_episodes=update_freq, return_rewards=True)\n",
    "# #     ep_rewards.append(ep_reward)\n",
    "#     spawn_agent(0, make_cart_pole, network.gen_act, mpmb, n_episodes=update_freq, max_steps=300, render=False)\n",
    "#     print(mpmb.get_avg_reward())\n",
    "#     train_data = mpmb.to_data()\n",
    "#     network.train(train_data)\n",
    "# #     if i % print_freq == 0 and i != 0:\n",
    "# #         print(f'Update #{int(i)}, Recent Reward:', np.mean(ep_rewards[print_freq:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
